{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6->2->6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomut\\anaconda3\\envs\\qhack2022\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\tomut\\anaconda3\\envs\\qhack2022\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path\n",
    "\n",
    "from qencode.initialize import setAB_amplitude, setAux, setEnt\n",
    "from qencode.encoders import e3_enhance\n",
    "from qencode.training_circuits import swap_t\n",
    "from qencode.qubits_arrangement import QubitsArrangement\n",
    "\n",
    "from qencode.utils.mnist import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data set size: 60000\n",
      "Final data set szize: 12665\n"
     ]
    }
   ],
   "source": [
    "input_data = get_dataset(img_width=8, img_height=8, train=True)\n",
    "print(\"Original data set size:\", len(input_data))\n",
    "\n",
    "# Select only the pictures with numbers 0 or 1. (jus to compare with literature)\n",
    "filtered_data = [image for image in input_data if image[1] in [0, 1]]\n",
    "input_data = filtered_data\n",
    "print(\"Final data set szize:\", len(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qubits: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "shots = 2500\n",
    "nr_trash=4\n",
    "nr_latent=2\n",
    "nr_ent=0\n",
    "nr_layers = 4\n",
    "\n",
    "spec = QubitsArrangement(nr_trash, nr_latent, nr_swap=1, nr_ent=nr_ent, nr_aux=nr_trash+nr_ent)\n",
    "print(\"Qubits:\", spec.qubits)\n",
    "\n",
    "#set up the device \n",
    "dev = qml.device(\"default.qubit\", wires=spec.num_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def training_circuit_example(init_params, encoder_params, reinit_state, x):\n",
    "    # Initialization\n",
    "    setAB_amplitude(spec, init_params)\n",
    "    setAux(spec, reinit_state)\n",
    "    setEnt(spec, inputs=[1 / np.sqrt(2), 0, 0, 1 / np.sqrt(2)])\n",
    "\n",
    "    #encoder \n",
    "    for params in encoder_params:\n",
    "        e3_enhance(params, x, spec)\n",
    "\n",
    "    #swap test \n",
    "    swap_t(spec)\n",
    "\n",
    "    return [qml.probs(i) for i in spec.swap_qubits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "learning_rate = 0.0003\n",
    "batch_size = 5\n",
    "num_samples = 20\n",
    "\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "opt = AdamOptimizer(learning_rate, beta1=beta1, beta2=beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fid_func(output):\n",
    "    # Implemented as the Fidelity Loss\n",
    "    # output[0] because we take the probability that the state after the \n",
    "    # SWAP test is ket(0), like the reference state\n",
    "    fidelity_loss = 1 / output[0]\n",
    "    return fidelity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(encoder_params, X):\n",
    "    reinit_state = [0 for _ in range(2 ** len(spec.aux_qubits))]\n",
    "    reinit_state[0] = 1.0\n",
    "    loss = 0.0\n",
    "    for x in X:\n",
    "        output = training_circuit_example(init_params=x[0][0],\n",
    "                                          encoder_params=encoder_params,\n",
    "                                          reinit_state=reinit_state, x=x[1])[0]\n",
    "        f = fid_func(output)\n",
    "        loss = loss + f\n",
    "    return loss / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fidelity(encoder_params, X):\n",
    "    reinit_state = [0 for _ in range(2 ** len(spec.aux_qubits))]\n",
    "    reinit_state[0] = 1.0\n",
    "    loss = 0.0\n",
    "    for x in X:\n",
    "        output = training_circuit_example(init_params=x[0][0],\n",
    "                                          encoder_params=encoder_params,\n",
    "                                          reinit_state=reinit_state, x=x[1])[0]\n",
    "        f = output[0]\n",
    "        loss = loss + f\n",
    "    return loss / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(X, batch_size):\n",
    "    X1 = [torch.reshape(x[0], (1, 2 ** (len(spec.latent_qubits) + len(spec.trash_qubits)))) for x in X]\n",
    "    X2 = []\n",
    "    for i in range(len(X1)):\n",
    "        X2.append([X1[1], X[i][1]])\n",
    "    X = X2\n",
    "    random.shuffle(X)\n",
    "\n",
    "    batch_list = []\n",
    "    batch = []\n",
    "    for x in X:\n",
    "        if len(batch) < batch_size:\n",
    "            batch.append(x)\n",
    "\n",
    "        else:\n",
    "            batch_list.append(batch)\n",
    "            batch = []\n",
    "    if len(batch) != 0:\n",
    "        batch_list.append(batch)\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [input_data[i] for i in range(num_samples)]\n",
    "test_data = [input_data[i] for i in range(num_samples,num_samples+num_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = [torch.reshape(x[0], (1, 2 ** (len(spec.latent_qubits) + len(spec.trash_qubits)))) for x in training_data]\n",
    "X2 = []\n",
    "for i in range(len(X1)):\n",
    "    X2.append([X1[1], training_data[i][1]])\n",
    "X_training = X2\n",
    "\n",
    "X1 = [torch.reshape(x[0], (1, 2 ** (len(spec.latent_qubits) + len(spec.trash_qubits)))) for x in test_data]\n",
    "X2 = []\n",
    "for i in range(len(X1)):\n",
    "    X2.append([X1[1], training_data[i][1]])\n",
    "X_tes = X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random encoder parameters\n",
    "\n",
    "nr_encod_qubits = len(spec.trash_qubits) + len(spec.latent_qubits)\n",
    "nr_par_encoder = nr_layers * 2 * nr_encod_qubits + 2 * len(spec.trash_qubits)\n",
    "encoder_params = np.random.uniform(size=(1, nr_par_encoder), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomut\\anaconda3\\envs\\qhack2022\\lib\\site-packages\\pennylane\\math\\multi_dispatch.py:63: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow and PyTorch over autograd. Consider replacing Autograd with vanilla NumPy.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 | Loss:1.9715840327381713 | Fidelity:0.5072215533111285\n",
      "Test-Epoch:0 | Loss:1.9584484595765161 | Fidelity:0.5106094979618462\n",
      "Epoch:5 | Loss:1.967848055995158 | Fidelity:0.5081824549222912\n",
      "Test-Epoch:5 | Loss:1.953516518468767 | Fidelity:0.511897684666687\n",
      "Epoch:10 | Loss:1.9635621093322417 | Fidelity:0.5092889172893889\n",
      "Test-Epoch:10 | Loss:1.9479557135661767 | Fidelity:0.5133587205685419\n",
      "Epoch:15 | Loss:1.9587329947641343 | Fidelity:0.5105415548337555\n",
      "Test-Epoch:15 | Loss:1.9418014190092787 | Fidelity:0.5149866888787928\n",
      "Epoch:20 | Loss:1.9531930190657463 | Fidelity:0.5119864683124258\n",
      "Test-Epoch:20 | Loss:1.9348366125136844 | Fidelity:0.5168434963324205\n",
      "Epoch:25 | Loss:1.9469830236177361 | Fidelity:0.5136167635679303\n",
      "Test-Epoch:25 | Loss:1.9271250912085993 | Fidelity:0.5189178433459507\n",
      "Epoch:30 | Loss:1.9400167865999596 | Fidelity:0.5154595267815345\n",
      "Test-Epoch:30 | Loss:1.9185541204102805 | Fidelity:0.5212471337425426\n",
      "Epoch:35 | Loss:1.9322866484803771 | Fidelity:0.5175224434897029\n",
      "Test-Epoch:35 | Loss:1.909137884233801 | Fidelity:0.5238359964206379\n",
      "Epoch:40 | Loss:1.9243288781031893 | Fidelity:0.5196663515953389\n",
      "Test-Epoch:40 | Loss:1.89959685075238 | Fidelity:0.5264902260432418\n",
      "Epoch:45 | Loss:1.9158410440110494 | Fidelity:0.5219763253692827\n",
      "Test-Epoch:45 | Loss:1.8895421334819393 | Fidelity:0.5293229891683711\n",
      "Epoch:50 | Loss:1.9069868878016187 | Fidelity:0.5244120290035816\n",
      "Test-Epoch:50 | Loss:1.879196037016105 | Fidelity:0.5322762286128768\n",
      "Epoch:55 | Loss:1.8976755039585096 | Fidelity:0.5270031060332034\n",
      "Test-Epoch:55 | Loss:1.86845661518616 | Fidelity:0.5353844373062369\n",
      "Epoch:60 | Loss:1.8878953417582918 | Fidelity:0.5297583932483664\n",
      "Test-Epoch:60 | Loss:1.8573232895164096 | Fidelity:0.5386540814458407\n",
      "Epoch:65 | Loss:1.8779480678087146 | Fidelity:0.5325962335752707\n",
      "Test-Epoch:65 | Loss:1.8461702788287095 | Fidelity:0.5419770205870547\n",
      "Epoch:70 | Loss:1.8677907038386266 | Fidelity:0.535531995414363\n",
      "Test-Epoch:70 | Loss:1.8349438052473388 | Fidelity:0.5453713354540517\n",
      "Epoch:75 | Loss:1.857533070846176 | Fidelity:0.5385361549049692\n",
      "Test-Epoch:75 | Loss:1.8237710504803832 | Fidelity:0.5487988671622672\n",
      "Epoch:80 | Loss:1.847496998750763 | Fidelity:0.5415122173101952\n",
      "Test-Epoch:80 | Loss:1.8130148333416467 | Fidelity:0.5521419574067776\n",
      "Epoch:85 | Loss:1.837758986927766 | Fidelity:0.5444337373153425\n",
      "Test-Epoch:85 | Loss:1.8027424782601909 | Fidelity:0.5553724235439919\n",
      "Epoch:90 | Loss:1.8281812694285229 | Fidelity:0.5473407377362174\n",
      "Test-Epoch:90 | Loss:1.7927781226388373 | Fidelity:0.5585427412506764\n",
      "Epoch:95 | Loss:1.8186669815344225 | Fidelity:0.5502631471950877\n",
      "Test-Epoch:95 | Loss:1.782998605162561 | Fidelity:0.5616917413053729\n",
      "Epoch:100 | Loss:1.809502502968557 | Fidelity:0.5531070630797645\n",
      "Test-Epoch:100 | Loss:1.7737078076388417 | Fidelity:0.5647115984854721\n",
      "Epoch:105 | Loss:1.8006362167623557 | Fidelity:0.5558856975560477\n",
      "Test-Epoch:105 | Loss:1.7648268025764142 | Fidelity:0.5676241342056123\n",
      "Epoch:110 | Loss:1.7919749812546062 | Fidelity:0.558627784377572\n",
      "Test-Epoch:110 | Loss:1.7562336850256866 | Fidelity:0.5704689353279084\n",
      "Epoch:115 | Loss:1.7836573206621242 | Fidelity:0.561283315592944\n",
      "Test-Epoch:115 | Loss:1.7480662237817248 | Fidelity:0.5731917515964776\n",
      "Epoch:120 | Loss:1.7757080311263684 | Fidelity:0.563840135273245\n",
      "Test-Epoch:120 | Loss:1.7403324093502612 | Fidelity:0.5757849616628824\n",
      "Epoch:125 | Loss:1.7679209925792256 | Fidelity:0.566368789966443\n",
      "Test-Epoch:125 | Loss:1.7327845724411137 | Fidelity:0.5783387671950897\n",
      "Epoch:130 | Loss:1.7604250305721476 | Fidelity:0.5688212045020478\n",
      "Test-Epoch:130 | Loss:1.7255544925705415 | Fidelity:0.5808003952952135\n",
      "Epoch:135 | Loss:1.7531028979767505 | Fidelity:0.5712399434384771\n",
      "Test-Epoch:135 | Loss:1.71848560356554 | Fidelity:0.583230127796437\n",
      "Epoch:140 | Loss:1.746112837699452 | Fidelity:0.5735611392523807\n",
      "Test-Epoch:140 | Loss:1.7117688340446793 | Fidelity:0.5855470468070529\n",
      "Epoch:145 | Loss:1.7393768247813615 | Fidelity:0.5758119033096862\n",
      "Test-Epoch:145 | Loss:1.7053001220896735 | Fidelity:0.5877901017185937\n",
      "Epoch:150 | Loss:1.7328571755218838 | Fidelity:0.5780060000106922\n",
      "Test-Epoch:150 | Loss:1.6990176474114762 | Fidelity:0.5899835371785687\n",
      "Epoch:155 | Loss:1.7265568377044702 | Fidelity:0.5801405922007602\n",
      "Test-Epoch:155 | Loss:1.6929190066407103 | Fidelity:0.5921267072239308\n",
      "Epoch:160 | Loss:1.7204563606891938 | Fidelity:0.5822229308072963\n",
      "Test-Epoch:160 | Loss:1.6869666092046842 | Fidelity:0.594234804721919\n",
      "Epoch:165 | Loss:1.7145727186879536 | Fidelity:0.5842387545979522\n",
      "Test-Epoch:165 | Loss:1.6812306079836783 | Fidelity:0.5962719403431923\n",
      "Epoch:170 | Loss:1.7088853942886335 | Fidelity:0.5861969066041339\n",
      "Test-Epoch:170 | Loss:1.675665448732643 | Fidelity:0.5982576928021784\n",
      "Epoch:175 | Loss:1.7033879901350015 | Fidelity:0.5880988896778314\n",
      "Test-Epoch:175 | Loss:1.6702623017481026 | Fidelity:0.6001949745350315\n",
      "Epoch:180 | Loss:1.6980603112377168 | Fidelity:0.5899572483761619\n",
      "Test-Epoch:180 | Loss:1.664944042894047 | Fidelity:0.602120206747021\n",
      "Epoch:185 | Loss:1.6928925773766128 | Fidelity:0.591758140201313\n",
      "Test-Epoch:185 | Loss:1.659863874372387 | Fidelity:0.6039535669979548\n",
      "Epoch:190 | Loss:1.6878883134084604 | Fidelity:0.5935146887804214\n",
      "Test-Epoch:190 | Loss:1.65487077925732 | Fidelity:0.6057712064652996\n",
      "Epoch:195 | Loss:1.6830253667202963 | Fidelity:0.5952250745503154\n",
      "Test-Epoch:195 | Loss:1.6500416261502384 | Fidelity:0.6075314575590489\n",
      "Epoch:200 | Loss:1.6783132447936608 | Fidelity:0.596893177524139\n",
      "Test-Epoch:200 | Loss:1.6453024665818698 | Fidelity:0.6092724420125611\n",
      "Epoch:205 | Loss:1.6737426043342283 | Fidelity:0.5985179424233522\n",
      "Test-Epoch:205 | Loss:1.6406911246749751 | Fidelity:0.6109741663604814\n",
      "Epoch:210 | Loss:1.669306052355117 | Fidelity:0.6000990380465103\n",
      "Test-Epoch:210 | Loss:1.6362291592708886 | Fidelity:0.6126245179031856\n",
      "Epoch:215 | Loss:1.6650130676270016 | Fidelity:0.6016383723384793\n",
      "Test-Epoch:215 | Loss:1.6318563653695652 | Fidelity:0.614254144186069\n",
      "Epoch:220 | Loss:1.660820778966451 | Fidelity:0.6031393967021708\n",
      "Test-Epoch:220 | Loss:1.6276784832838804 | Fidelity:0.6158055422061384\n",
      "Epoch:225 | Loss:1.6567676045854534 | Fidelity:0.6045995329400511\n",
      "Test-Epoch:225 | Loss:1.6235800779846852 | Fidelity:0.6173393655058462\n",
      "Epoch:230 | Loss:1.6528644442115958 | Fidelity:0.6060173684097921\n",
      "Test-Epoch:230 | Loss:1.6195354369070043 | Fidelity:0.6188696992095502\n",
      "Epoch:235 | Loss:1.6491018631243093 | Fidelity:0.6073933059417492\n",
      "Test-Epoch:235 | Loss:1.615572288957911 | Fidelity:0.6203819505398915\n",
      "Epoch:240 | Loss:1.645429476785237 | Fidelity:0.6087356881913497\n",
      "Test-Epoch:240 | Loss:1.6117846623272407 | Fidelity:0.6218246070909345\n",
      "Epoch:245 | Loss:1.641916260627415 | Fidelity:0.6100320588867656\n",
      "Test-Epoch:245 | Loss:1.6080443021336361 | Fidelity:0.6232671507324069\n",
      "Epoch:250 | Loss:1.6384271205851708 | Fidelity:0.6113099507564549\n",
      "Test-Epoch:250 | Loss:1.6045452629907726 | Fidelity:0.6245998709851135\n",
      "Epoch:255 | Loss:1.6350628053049303 | Fidelity:0.6125486985579705\n",
      "Test-Epoch:255 | Loss:1.6011249233381772 | Fidelity:0.625911690686075\n",
      "Epoch:260 | Loss:1.6318287834052232 | Fidelity:0.6137481057333378\n",
      "Test-Epoch:260 | Loss:1.5977540478519818 | Fidelity:0.6272173111674609\n",
      "Epoch:265 | Loss:1.6286496578139236 | Fidelity:0.6149250077422691\n",
      "Test-Epoch:265 | Loss:1.5945392236277645 | Fidelity:0.6284571686285306\n",
      "Epoch:270 | Loss:1.6255894470027914 | Fidelity:0.6160649690139797\n",
      "Test-Epoch:270 | Loss:1.5913834646425644 | Fidelity:0.6296845457357613\n",
      "Epoch:275 | Loss:1.6226171209351445 | Fidelity:0.6171751439432213\n",
      "Test-Epoch:275 | Loss:1.5883287274776803 | Fidelity:0.63087587618124\n",
      "Epoch:280 | Loss:1.6196979368657505 | Fidelity:0.6182642473914597\n",
      "Test-Epoch:280 | Loss:1.5854095284588339 | Fidelity:0.6320103836496754\n",
      "Epoch:285 | Loss:1.6168610192364679 | Fidelity:0.6193262842364373\n",
      "Test-Epoch:285 | Loss:1.5825637381574296 | Fidelity:0.6331207721738032\n",
      "Epoch:290 | Loss:1.614111199104303 | Fidelity:0.6203601187815091\n",
      "Test-Epoch:290 | Loss:1.5797810849002478 | Fidelity:0.6342124234163714\n",
      "Epoch:295 | Loss:1.6114556045775412 | Fidelity:0.6213644232654798\n",
      "Test-Epoch:295 | Loss:1.5770452629791032 | Fidelity:0.6352942257370767\n",
      "Epoch:300 | Loss:1.608887163260513 | Fidelity:0.6223408958452087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-Epoch:300 | Loss:1.5743643225201691 | Fidelity:0.6363615762248424\n",
      "Epoch:305 | Loss:1.6063783051695275 | Fidelity:0.6232965705549888\n",
      "Test-Epoch:305 | Loss:1.5717781575675063 | Fidelity:0.6373925146533959\n",
      "Epoch:310 | Loss:1.6039444002395737 | Fidelity:0.6242280243684448\n",
      "Test-Epoch:310 | Loss:1.5692475660408525 | Fidelity:0.6384071743978648\n",
      "Epoch:315 | Loss:1.6015312720092836 | Fidelity:0.6251494491403445\n",
      "Test-Epoch:315 | Loss:1.5668374036087538 | Fidelity:0.6393681056208426\n",
      "Epoch:320 | Loss:1.5991757357328655 | Fidelity:0.6260518497360993\n",
      "Test-Epoch:320 | Loss:1.564482911610634 | Fidelity:0.6403102443505747\n",
      "Epoch:325 | Loss:1.5968751369360554 | Fidelity:0.6269360882343096\n",
      "Test-Epoch:325 | Loss:1.5621798721314986 | Fidelity:0.6412351634978402\n",
      "Epoch:330 | Loss:1.594642983681322 | Fidelity:0.6277990548959643\n",
      "Test-Epoch:330 | Loss:1.5599023069595295 | Fidelity:0.6421571308565356\n",
      "Epoch:335 | Loss:1.592428420655295 | Fidelity:0.6286541346531529\n",
      "Test-Epoch:335 | Loss:1.557720739494631 | Fidelity:0.6430363678069293\n",
      "Epoch:340 | Loss:1.5902855155085343 | Fidelity:0.629488091125153\n",
      "Test-Epoch:340 | Loss:1.5555353433061796 | Fidelity:0.6439272942376127\n",
      "Epoch:345 | Loss:1.588180946728631 | Fidelity:0.6303093098806428\n",
      "Test-Epoch:345 | Loss:1.5534050454245838 | Fidelity:0.6447977735107626\n",
      "Epoch:350 | Loss:1.5861605153600202 | Fidelity:0.6311071963335384\n",
      "Test-Epoch:350 | Loss:1.5512435447252297 | Fidelity:0.6456960099498816\n",
      "Epoch:355 | Loss:1.5841319628740729 | Fidelity:0.6319046407911357\n",
      "Test-Epoch:355 | Loss:1.5492089144626986 | Fidelity:0.6465337250897468\n",
      "Epoch:360 | Loss:1.5821435663860814 | Fidelity:0.6326904026834183\n",
      "Test-Epoch:360 | Loss:1.547196025231095 | Fidelity:0.6473678245339725\n",
      "Epoch:365 | Loss:1.5801768548706814 | Fidelity:0.6334690980938591\n",
      "Test-Epoch:365 | Loss:1.5452321808064158 | Fidelity:0.6481824180440962\n",
      "Epoch:370 | Loss:1.5782008651154926 | Fidelity:0.6342477625304205\n",
      "Test-Epoch:370 | Loss:1.5433710391913036 | Fidelity:0.6489461856459848\n",
      "Epoch:375 | Loss:1.5762708867435937 | Fidelity:0.6350144345546301\n",
      "Test-Epoch:375 | Loss:1.5414850379808867 | Fidelity:0.64972956717637\n",
      "Epoch:380 | Loss:1.5743728906024468 | Fidelity:0.6357727454849502\n",
      "Test-Epoch:380 | Loss:1.5396005481019597 | Fidelity:0.6505181518772429\n",
      "Epoch:385 | Loss:1.5724786319447188 | Fidelity:0.6365277100153031\n",
      "Test-Epoch:385 | Loss:1.5377976471520827 | Fidelity:0.651267632910279\n",
      "Epoch:390 | Loss:1.570619884949379 | Fidelity:0.6372750158177029\n",
      "Test-Epoch:390 | Loss:1.5359651847176878 | Fidelity:0.652039027679308\n",
      "Epoch:395 | Loss:1.568781501971335 | Fidelity:0.6380177228467871\n",
      "Test-Epoch:395 | Loss:1.5341439960123204 | Fidelity:0.6528099289881183\n",
      "Epoch:400 | Loss:1.5669647494461267 | Fidelity:0.6387571522032676\n",
      "Test-Epoch:400 | Loss:1.5323170328932556 | Fidelity:0.6535904891381945\n",
      "Epoch:405 | Loss:1.565157485825714 | Fidelity:0.6394948113129002\n",
      "Test-Epoch:405 | Loss:1.5305230165555401 | Fidelity:0.6543585826817687\n",
      "Epoch:410 | Loss:1.5633601224511902 | Fidelity:0.6402313866395064\n",
      "Test-Epoch:410 | Loss:1.5287543713544802 | Fidelity:0.6551187003766372\n",
      "Epoch:415 | Loss:1.5615603921137917 | Fidelity:0.6409652300814281\n",
      "Test-Epoch:415 | Loss:1.5270717838520171 | Fidelity:0.6558344099040696\n",
      "Epoch:420 | Loss:1.559776840191501 | Fidelity:0.6417004936422499\n",
      "Test-Epoch:420 | Loss:1.5253470825613058 | Fidelity:0.6565792344879486\n",
      "Epoch:425 | Loss:1.5580012231214226 | Fidelity:0.642436088506208\n",
      "Test-Epoch:425 | Loss:1.523627062862704 | Fidelity:0.6573259516716892\n",
      "Epoch:430 | Loss:1.5562278380738541 | Fidelity:0.6431610170145248\n",
      "Test-Epoch:430 | Loss:1.5220445366924031 | Fidelity:0.6579967048587274\n",
      "Epoch:435 | Loss:1.5544680646508198 | Fidelity:0.6438876553021947\n",
      "Test-Epoch:435 | Loss:1.5204246694810002 | Fidelity:0.6586934506265794\n",
      "Epoch:440 | Loss:1.552717290179151 | Fidelity:0.6446144361868857\n",
      "Test-Epoch:440 | Loss:1.5187936328265 | Fidelity:0.6593995989375503\n",
      "Epoch:445 | Loss:1.550977258679136 | Fidelity:0.6453373818840519\n",
      "Test-Epoch:445 | Loss:1.5171869421317228 | Fidelity:0.6600947809902813\n",
      "Epoch:450 | Loss:1.5492470942757266 | Fidelity:0.6460585282262928\n",
      "Test-Epoch:450 | Loss:1.5155875651387474 | Fidelity:0.6607889095784062\n",
      "Epoch:455 | Loss:1.5474967075315944 | Fidelity:0.6468030046338153\n",
      "Test-Epoch:455 | Loss:1.5138839839344376 | Fidelity:0.661548927843765\n",
      "Epoch:460 | Loss:1.5457446596175095 | Fidelity:0.6475527667193979\n",
      "Test-Epoch:460 | Loss:1.512184351068609 | Fidelity:0.662311997692877\n",
      "Epoch:465 | Loss:1.544009741889502 | Fidelity:0.6482900011091882\n",
      "Test-Epoch:465 | Loss:1.5105710471044698 | Fidelity:0.6630275302421438\n",
      "Epoch:470 | Loss:1.5422785948894588 | Fidelity:0.649027721531705\n",
      "Test-Epoch:470 | Loss:1.5089866872888387 | Fidelity:0.6637314293013656\n",
      "Epoch:475 | Loss:1.5405452019631238 | Fidelity:0.649771033685995\n",
      "Test-Epoch:475 | Loss:1.5074034851178708 | Fidelity:0.664439665898793\n",
      "Epoch:480 | Loss:1.538821699943842 | Fidelity:0.6505107650011529\n",
      "Test-Epoch:480 | Loss:1.5058381962594638 | Fidelity:0.6651393447847196\n",
      "Epoch:485 | Loss:1.5370970081470066 | Fidelity:0.6512557155095653\n",
      "Test-Epoch:485 | Loss:1.504273417105115 | Fidelity:0.6658436992600807\n",
      "Epoch:490 | Loss:1.5354152061107236 | Fidelity:0.651972719540926\n",
      "Test-Epoch:490 | Loss:1.5027742195518752 | Fidelity:0.6665046949721702\n",
      "Epoch:495 | Loss:1.5337220082822096 | Fidelity:0.6527032403432801\n",
      "Test-Epoch:495 | Loss:1.501259669177758 | Fidelity:0.66718288189325\n",
      "Epoch:500 | Loss:1.5320203545408853 | Fidelity:0.653444775133921\n",
      "Test-Epoch:500 | Loss:1.4997190012391242 | Fidelity:0.6678814064022147\n",
      "Epoch:505 | Loss:1.530355320372308 | Fidelity:0.6541649739329581\n",
      "Test-Epoch:505 | Loss:1.498207401068375 | Fidelity:0.6685586055292904\n",
      "Epoch:510 | Loss:1.5287213918380058 | Fidelity:0.6548690873332303\n",
      "Test-Epoch:510 | Loss:1.496732579140675 | Fidelity:0.6692148907795624\n",
      "Epoch:515 | Loss:1.5270857355983625 | Fidelity:0.6555799354055449\n",
      "Test-Epoch:515 | Loss:1.495258410800915 | Fidelity:0.6698777078427887\n",
      "Epoch:520 | Loss:1.5254435560161554 | Fidelity:0.6563004860108578\n",
      "Test-Epoch:520 | Loss:1.4937700074435567 | Fidelity:0.6705548760032307\n",
      "Epoch:525 | Loss:1.5238301757423824 | Fidelity:0.6570063823348489\n",
      "Test-Epoch:525 | Loss:1.492291853212163 | Fidelity:0.6712239725217353\n",
      "Epoch:530 | Loss:1.5222079290481674 | Fidelity:0.6577233330085501\n",
      "Test-Epoch:530 | Loss:1.4908136859388283 | Fidelity:0.6719010970294776\n",
      "Epoch:535 | Loss:1.5206348274640806 | Fidelity:0.6584115451236487\n",
      "Test-Epoch:535 | Loss:1.4893556379871848 | Fidelity:0.6725594332104341\n",
      "Epoch:540 | Loss:1.5190870186826393 | Fidelity:0.6590878945916043\n",
      "Test-Epoch:540 | Loss:1.4879333575091418 | Fidelity:0.6731997929999551\n",
      "Epoch:545 | Loss:1.517524003636172 | Fidelity:0.6597797368269467\n",
      "Test-Epoch:545 | Loss:1.4865175738069696 | Fidelity:0.6738472712736494\n",
      "Epoch:550 | Loss:1.5159778829724728 | Fidelity:0.6604649475421642\n",
      "Test-Epoch:550 | Loss:1.485099088458383 | Fidelity:0.6744964789086142\n",
      "Epoch:555 | Loss:1.514447171464383 | Fidelity:0.6611442679259283\n",
      "Test-Epoch:555 | Loss:1.4836903413211662 | Fidelity:0.6751417256900901\n",
      "Epoch:560 | Loss:1.5129166299435304 | Fidelity:0.6618283761125194\n",
      "Test-Epoch:560 | Loss:1.4822964339101485 | Fidelity:0.6757852770080663\n",
      "Epoch:565 | Loss:1.5114002284411496 | Fidelity:0.6625066946583563\n",
      "Test-Epoch:565 | Loss:1.48090269952006 | Fidelity:0.6764287363947036\n",
      "Epoch:570 | Loss:1.5099171172798014 | Fidelity:0.6631653314990995\n",
      "Test-Epoch:570 | Loss:1.4795210815880189 | Fidelity:0.6770603090992597\n",
      "Epoch:575 | Loss:1.5084074854949432 | Fidelity:0.6638472419400269\n",
      "Test-Epoch:575 | Loss:1.4781878120199812 | Fidelity:0.6776820049835119\n",
      "Epoch:580 | Loss:1.5069184089934593 | Fidelity:0.664517678490502\n",
      "Test-Epoch:580 | Loss:1.4768323786639033 | Fidelity:0.6783109458762381\n",
      "Epoch:585 | Loss:1.5054452357040335 | Fidelity:0.6651793353193145\n",
      "Test-Epoch:585 | Loss:1.4754836295395357 | Fidelity:0.6789343056099766\n",
      "Epoch:590 | Loss:1.5039900067615242 | Fidelity:0.6658306325371596\n",
      "Test-Epoch:590 | Loss:1.4741520018403136 | Fidelity:0.6795465244431246\n",
      "Epoch:595 | Loss:1.5025191059375194 | Fidelity:0.6664959706272438\n",
      "Test-Epoch:595 | Loss:1.4728673203748734 | Fidelity:0.6801440693358725\n",
      "Epoch:600 | Loss:1.50103530664289 | Fidelity:0.6671726389099532\n",
      "Test-Epoch:600 | Loss:1.4715896244464362 | Fidelity:0.6807440416963784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:605 | Loss:1.4995705594503141 | Fidelity:0.6678349002864395\n",
      "Test-Epoch:605 | Loss:1.4702763131573537 | Fidelity:0.6813537926904872\n",
      "Epoch:610 | Loss:1.498095534564629 | Fidelity:0.6685048317508823\n",
      "Test-Epoch:610 | Loss:1.4690000975396833 | Fidelity:0.6819486845730177\n",
      "Epoch:615 | Loss:1.4966070011324284 | Fidelity:0.6691838361117385\n",
      "Test-Epoch:615 | Loss:1.467738559366188 | Fidelity:0.6825392336027725\n",
      "Epoch:620 | Loss:1.4951245485022142 | Fidelity:0.6698563010817395\n",
      "Test-Epoch:620 | Loss:1.4664574954699976 | Fidelity:0.6831343186814296\n",
      "Epoch:625 | Loss:1.4936106169391976 | Fidelity:0.6705496098241968\n",
      "Test-Epoch:625 | Loss:1.4652202441217843 | Fidelity:0.683714911306189\n",
      "Epoch:630 | Loss:1.4920985020212192 | Fidelity:0.6712377006184334\n",
      "Test-Epoch:630 | Loss:1.4639483342987092 | Fidelity:0.6843066019487237\n",
      "Epoch:635 | Loss:1.4905676279328859 | Fidelity:0.6719356155355286\n",
      "Test-Epoch:635 | Loss:1.4626826162358326 | Fidelity:0.6848960100592942\n",
      "Epoch:640 | Loss:1.4890239002001613 | Fidelity:0.6726386440590693\n",
      "Test-Epoch:640 | Loss:1.4614256321383028 | Fidelity:0.6854796761365606\n",
      "Epoch:645 | Loss:1.487432535263841 | Fidelity:0.673370045181357\n",
      "Test-Epoch:645 | Loss:1.4601996289409518 | Fidelity:0.6860545249156329\n",
      "Epoch:650 | Loss:1.4858178251746015 | Fidelity:0.6741121460696377\n",
      "Test-Epoch:650 | Loss:1.458950297877707 | Fidelity:0.6866396923508005\n",
      "Epoch:655 | Loss:1.4841249495354485 | Fidelity:0.6748998616433459\n",
      "Test-Epoch:655 | Loss:1.4577394865234263 | Fidelity:0.6872149720716081\n",
      "Epoch:660 | Loss:1.4823823898014485 | Fidelity:0.6757132048167023\n",
      "Test-Epoch:660 | Loss:1.4564911755208036 | Fidelity:0.6878097482784302\n",
      "Epoch:665 | Loss:1.480633630078183 | Fidelity:0.676524340262395\n",
      "Test-Epoch:665 | Loss:1.455181442562718 | Fidelity:0.6884278532817946\n",
      "Epoch:670 | Loss:1.4787567620055675 | Fidelity:0.6774089736481255\n",
      "Test-Epoch:670 | Loss:1.4539454072622378 | Fidelity:0.6890223654211203\n",
      "Epoch:675 | Loss:1.476792997133553 | Fidelity:0.6783398736625121\n",
      "Test-Epoch:675 | Loss:1.4526705503116868 | Fidelity:0.6896394428547313\n",
      "Epoch:680 | Loss:1.474812297544225 | Fidelity:0.6792737910156568\n",
      "Test-Epoch:680 | Loss:1.4512990746787338 | Fidelity:0.6902969592596716\n",
      "Epoch:685 | Loss:1.47273806146837 | Fidelity:0.6802572909748015\n",
      "Test-Epoch:685 | Loss:1.4499275795458761 | Fidelity:0.6909579182135086\n",
      "Epoch:690 | Loss:1.4705472459965214 | Fidelity:0.68130352622226\n",
      "Test-Epoch:690 | Loss:1.4485369891191808 | Fidelity:0.6916333902143045\n",
      "Epoch:695 | Loss:1.4682465851541588 | Fidelity:0.682407758789082\n",
      "Test-Epoch:695 | Loss:1.4470825381444912 | Fidelity:0.6923434301502447\n",
      "Epoch:700 | Loss:1.4658324896764174 | Fidelity:0.6835717719897951\n",
      "Test-Epoch:700 | Loss:1.4455554028911008 | Fidelity:0.6930920826820237\n",
      "Epoch:705 | Loss:1.4633026038612367 | Fidelity:0.684797094270846\n",
      "Test-Epoch:705 | Loss:1.4439536016460166 | Fidelity:0.6938803787899311\n",
      "Epoch:710 | Loss:1.4606478481209093 | Fidelity:0.6860893473317846\n",
      "Test-Epoch:710 | Loss:1.4422661240161552 | Fidelity:0.6947146373424633\n",
      "Epoch:715 | Loss:1.4579194490153633 | Fidelity:0.6874200677218681\n",
      "Test-Epoch:715 | Loss:1.4404811602864596 | Fidelity:0.6955972967274201\n",
      "Epoch:720 | Loss:1.4550618052197344 | Fidelity:0.688821990703616\n",
      "Test-Epoch:720 | Loss:1.4386091142676798 | Fidelity:0.696528309669233\n",
      "Epoch:725 | Loss:1.4520933557697873 | Fidelity:0.6902857411723696\n",
      "Test-Epoch:725 | Loss:1.4366367340085227 | Fidelity:0.6975137320273709\n",
      "Epoch:730 | Loss:1.4488830480103823 | Fidelity:0.691885670319447\n",
      "Test-Epoch:730 | Loss:1.4345310995782516 | Fidelity:0.6985784174177833\n",
      "Epoch:735 | Loss:1.445461556275829 | Fidelity:0.6936056837423499\n",
      "Test-Epoch:735 | Loss:1.4322591142112728 | Fidelity:0.6997373423879798\n",
      "Epoch:740 | Loss:1.4418690386179733 | Fidelity:0.6954247843914403\n",
      "Test-Epoch:740 | Loss:1.42981503593039 | Fidelity:0.7009920913959168\n",
      "Epoch:745 | Loss:1.438054661002474 | Fidelity:0.6973739585543692\n",
      "Test-Epoch:745 | Loss:1.427180740897978 | Fidelity:0.7023564154481402\n",
      "Epoch:750 | Loss:1.4341629033634191 | Fidelity:0.6993736082064632\n",
      "Test-Epoch:750 | Loss:1.4243973975379536 | Fidelity:0.703803323903232\n",
      "Epoch:755 | Loss:1.430129160607993 | Fidelity:0.7014626063859088\n",
      "Test-Epoch:755 | Loss:1.421446952778414 | Fidelity:0.7053478173054251\n",
      "Epoch:760 | Loss:1.4260407808035303 | Fidelity:0.7035929176954206\n",
      "Test-Epoch:760 | Loss:1.4183662951873395 | Fidelity:0.7069682680934459\n",
      "Epoch:765 | Loss:1.4220405469191193 | Fidelity:0.7056831290352535\n",
      "Test-Epoch:765 | Loss:1.4152389751038945 | Fidelity:0.7086153030726277\n",
      "Epoch:770 | Loss:1.4178451223153647 | Fidelity:0.7078995884704866\n",
      "Test-Epoch:770 | Loss:1.411923992621111 | Fidelity:0.7103804233985425\n",
      "Epoch:775 | Loss:1.4135800552715998 | Fidelity:0.7101712972693186\n",
      "Test-Epoch:775 | Loss:1.4084863763879882 | Fidelity:0.712224813436743\n",
      "Epoch:780 | Loss:1.409238867025785 | Fidelity:0.7125041914999879\n",
      "Test-Epoch:780 | Loss:1.4049226370580465 | Fidelity:0.7141531565900341\n",
      "Epoch:785 | Loss:1.404881729984257 | Fidelity:0.714863831613734\n",
      "Test-Epoch:785 | Loss:1.401269651415451 | Fidelity:0.7161442873942253\n",
      "Epoch:790 | Loss:1.400571039412884 | Fidelity:0.7172136015059916\n",
      "Test-Epoch:790 | Loss:1.3975768304258058 | Fidelity:0.7181696580083412\n",
      "Epoch:795 | Loss:1.3961204401324447 | Fidelity:0.7196696974726684\n",
      "Test-Epoch:795 | Loss:1.3937164622117393 | Fidelity:0.7203134168179867\n",
      "Epoch:800 | Loss:1.3918634327068369 | Fidelity:0.7220259259099568\n",
      "Test-Epoch:800 | Loss:1.3899402538033976 | Fidelity:0.7224164870467685\n",
      "Epoch:805 | Loss:1.3877532401186072 | Fidelity:0.7243098720636876\n",
      "Test-Epoch:805 | Loss:1.3862166400849094 | Fidelity:0.7244989365875695\n",
      "Epoch:810 | Loss:1.3837409312325917 | Fidelity:0.726551651079339\n",
      "Test-Epoch:810 | Loss:1.3825162545323255 | Fidelity:0.726580656039063\n",
      "Epoch:815 | Loss:1.379904818237318 | Fidelity:0.7286988525963994\n",
      "Test-Epoch:815 | Loss:1.3789165846407254 | Fidelity:0.7286108524578754\n",
      "Epoch:820 | Loss:1.3761287786601488 | Fidelity:0.7308276533435585\n",
      "Test-Epoch:820 | Loss:1.3753149679619778 | Fidelity:0.7306580527550641\n",
      "Epoch:825 | Loss:1.3724481359667942 | Fidelity:0.7329141966180316\n",
      "Test-Epoch:825 | Loss:1.3717540619712125 | Fidelity:0.7326950405637358\n",
      "Epoch:830 | Loss:1.3688854380509523 | Fidelity:0.734942114999133\n",
      "Test-Epoch:830 | Loss:1.3682650035076143 | Fidelity:0.7347011673591765\n",
      "Epoch:835 | Loss:1.3655222875772786 | Fidelity:0.7368528119771895\n",
      "Test-Epoch:835 | Loss:1.36492855279434 | Fidelity:0.736618863891816\n",
      "Epoch:840 | Loss:1.362220799041252 | Fidelity:0.7387436810455369\n",
      "Test-Epoch:840 | Loss:1.361605915829805 | Fidelity:0.738545510722427\n",
      "Epoch:845 | Loss:1.3590098663408083 | Fidelity:0.7405947512495857\n",
      "Test-Epoch:845 | Loss:1.358336147509134 | Fidelity:0.7404557192102292\n",
      "Epoch:850 | Loss:1.3560265555956312 | Fidelity:0.7422957885710237\n",
      "Test-Epoch:850 | Loss:1.3552936049993822 | Fidelity:0.7422185304298413\n",
      "Epoch:855 | Loss:1.3531311059951856 | Fidelity:0.7439578904595152\n",
      "Test-Epoch:855 | Loss:1.3522882484760212 | Fidelity:0.7439729081219799\n",
      "Epoch:860 | Loss:1.3503041346212037 | Fidelity:0.7455996145440138\n",
      "Test-Epoch:860 | Loss:1.3493070069478343 | Fidelity:0.7457338025646011\n",
      "Epoch:865 | Loss:1.3476268459453487 | Fidelity:0.7471456655285579\n",
      "Test-Epoch:865 | Loss:1.3464884383129505 | Fidelity:0.7473931007724339\n",
      "Epoch:870 | Loss:1.3450995385755715 | Fidelity:0.7485858457963788\n",
      "Test-Epoch:870 | Loss:1.3438398445892696 | Fidelity:0.7489362785024405\n",
      "Epoch:875 | Loss:1.342678626747794 | Fidelity:0.7499611768249561\n",
      "Test-Epoch:875 | Loss:1.341276638545723 | Fidelity:0.7504276144956723\n",
      "Epoch:880 | Loss:1.340349337640254 | Fidelity:0.7512861673920803\n",
      "Test-Epoch:880 | Loss:1.3387766351640538 | Fidelity:0.7518856997250642\n",
      "Epoch:885 | Loss:1.3381040303330483 | Fidelity:0.7525681745849426\n",
      "Test-Epoch:885 | Loss:1.336341871515105 | Fidelity:0.7533120519485877\n",
      "Epoch:890 | Loss:1.335946061224361 | Fidelity:0.7537986238856156\n",
      "Test-Epoch:890 | Loss:1.3339949713783616 | Fidelity:0.754686720501765\n",
      "Epoch:895 | Loss:1.3338652799345936 | Fidelity:0.7549718170838687\n",
      "Test-Epoch:895 | Loss:1.3317489776791755 | Fidelity:0.7559904974963142\n",
      "Epoch:900 | Loss:1.3318625525226797 | Fidelity:0.7561054020947526\n",
      "Test-Epoch:900 | Loss:1.3295589702752229 | Fidelity:0.7572673664347125\n",
      "Epoch:905 | Loss:1.3299208869644383 | Fidelity:0.7571940069242806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-Epoch:905 | Loss:1.327453172421553 | Fidelity:0.7584855508356672\n",
      "Epoch:910 | Loss:1.3280683253003525 | Fidelity:0.7582442795066167\n",
      "Test-Epoch:910 | Loss:1.3253988850084364 | Fidelity:0.7596867025043734\n",
      "Epoch:915 | Loss:1.326293479734221 | Fidelity:0.7592529315344174\n",
      "Test-Epoch:915 | Loss:1.3234298159688038 | Fidelity:0.7608408519095123\n",
      "Epoch:920 | Loss:1.324557365929251 | Fidelity:0.7602250280307898\n",
      "Test-Epoch:920 | Loss:1.32154341533387 | Fidelity:0.7619321116937676\n",
      "Epoch:925 | Loss:1.3228801505286805 | Fidelity:0.7611620971513282\n",
      "Test-Epoch:925 | Loss:1.3197155434055015 | Fidelity:0.762987790351656\n",
      "Epoch:930 | Loss:1.3212139118569204 | Fidelity:0.7620764161733695\n",
      "Test-Epoch:930 | Loss:1.3179348721969013 | Fidelity:0.7639992059736318\n",
      "Epoch:935 | Loss:1.3195994902669763 | Fidelity:0.7629617731374121\n",
      "Test-Epoch:935 | Loss:1.3161879232375093 | Fidelity:0.7649916593409996\n",
      "Epoch:940 | Loss:1.3180643134815178 | Fidelity:0.7638116724323106\n",
      "Test-Epoch:940 | Loss:1.3144911414857012 | Fidelity:0.7659644248674418\n",
      "Epoch:945 | Loss:1.3165874237044237 | Fidelity:0.7646303950741513\n",
      "Test-Epoch:945 | Loss:1.3128626095682592 | Fidelity:0.7668985407434361\n",
      "Epoch:950 | Loss:1.3150935002309878 | Fidelity:0.7654417308650302\n",
      "Test-Epoch:950 | Loss:1.3112615671820302 | Fidelity:0.7677987687748372\n",
      "Epoch:955 | Loss:1.3136451578298705 | Fidelity:0.7662282761742836\n",
      "Test-Epoch:955 | Loss:1.309694905137302 | Fidelity:0.7686799397581897\n",
      "Epoch:960 | Loss:1.3122255128636202 | Fidelity:0.7669962386692233\n",
      "Test-Epoch:960 | Loss:1.3081589838501817 | Fidelity:0.7695404748035302\n",
      "Epoch:965 | Loss:1.3108694530034826 | Fidelity:0.7677343123981482\n",
      "Test-Epoch:965 | Loss:1.30667810241618 | Fidelity:0.7703744638675389\n",
      "Epoch:970 | Loss:1.309521597943164 | Fidelity:0.7684615073901327\n",
      "Test-Epoch:970 | Loss:1.3052255651700186 | Fidelity:0.7711848436007415\n",
      "Epoch:975 | Loss:1.3082395777677258 | Fidelity:0.7691574662407651\n",
      "Test-Epoch:975 | Loss:1.3038359346763466 | Fidelity:0.7719638303530464\n",
      "Epoch:980 | Loss:1.306941775618048 | Fidelity:0.7698530529940655\n",
      "Test-Epoch:980 | Loss:1.302454493999273 | Fidelity:0.772727839732575\n",
      "Epoch:985 | Loss:1.3056873514195735 | Fidelity:0.7705268817625015\n",
      "Test-Epoch:985 | Loss:1.3011095035148417 | Fidelity:0.7734729945731906\n",
      "Epoch:990 | Loss:1.3044964105933317 | Fidelity:0.7711706180117702\n",
      "Test-Epoch:990 | Loss:1.2998277971170569 | Fidelity:0.7741861585158148\n",
      "Epoch:995 | Loss:1.3032975594391765 | Fidelity:0.7718123647991166\n",
      "Test-Epoch:995 | Loss:1.2985635786213408 | Fidelity:0.7748814512767538\n",
      "Epoch:1000 | Loss:1.302127933349096 | Fidelity:0.7724380077237432\n",
      "Test-Epoch:1000 | Loss:1.2973312447912715 | Fidelity:0.7755580088513054\n",
      "Epoch:1005 | Loss:1.300930129386323 | Fidelity:0.7730720403777743\n",
      "Test-Epoch:1005 | Loss:1.2960823463215208 | Fidelity:0.7762361638537527\n",
      "Epoch:1010 | Loss:1.2998507456596282 | Fidelity:0.7736532842570567\n",
      "Test-Epoch:1010 | Loss:1.2949348160117609 | Fidelity:0.7768688683846143\n",
      "Epoch:1015 | Loss:1.298689518233264 | Fidelity:0.77426631139082\n",
      "Test-Epoch:1015 | Loss:1.2937347840101052 | Fidelity:0.7775166754521956\n",
      "Epoch:1020 | Loss:1.297632362833498 | Fidelity:0.7748321493469977\n",
      "Test-Epoch:1020 | Loss:1.2926245303923725 | Fidelity:0.7781234580716319\n",
      "Epoch:1025 | Loss:1.2965983016088225 | Fidelity:0.7753848295534082\n",
      "Test-Epoch:1025 | Loss:1.2915525022642038 | Fidelity:0.7787067139192911\n",
      "Epoch:1030 | Loss:1.2955141380355613 | Fidelity:0.7759575248714798\n",
      "Test-Epoch:1030 | Loss:1.2904442277106158 | Fidelity:0.7793022181208606\n",
      "Epoch:1035 | Loss:1.2944812484527923 | Fidelity:0.7765060286114562\n",
      "Test-Epoch:1035 | Loss:1.2893781289196278 | Fidelity:0.7798778858395508\n",
      "Epoch:1040 | Loss:1.2934315254640245 | Fidelity:0.7770608199568527\n",
      "Test-Epoch:1040 | Loss:1.2883020135835654 | Fidelity:0.7804555835944477\n",
      "Epoch:1045 | Loss:1.2923411323498049 | Fidelity:0.7776335030174291\n",
      "Test-Epoch:1045 | Loss:1.2871833860886974 | Fidelity:0.7810529927110685\n",
      "Epoch:1050 | Loss:1.2912947860263375 | Fidelity:0.7781860454543896\n",
      "Test-Epoch:1050 | Loss:1.2860987596423858 | Fidelity:0.7816353459249471\n",
      "Epoch:1055 | Loss:1.2902353817390848 | Fidelity:0.7787444896356575\n",
      "Test-Epoch:1055 | Loss:1.284999987760235 | Fidelity:0.782224209623456\n",
      "Epoch:1060 | Loss:1.28916140156525 | Fidelity:0.7793099651297191\n",
      "Test-Epoch:1060 | Loss:1.2838815156050412 | Fidelity:0.7828233444190753\n",
      "Epoch:1065 | Loss:1.2881613781746033 | Fidelity:0.779841631773823\n",
      "Test-Epoch:1065 | Loss:1.2828330094909213 | Fidelity:0.7833892781425364\n",
      "Epoch:1070 | Loss:1.287070907427131 | Fidelity:0.7804154766480702\n",
      "Test-Epoch:1070 | Loss:1.2816978261215586 | Fidelity:0.7839965822452741\n",
      "Epoch:1075 | Loss:1.2860379540339417 | Fidelity:0.7809634749590526\n",
      "Test-Epoch:1075 | Loss:1.2806117717607473 | Fidelity:0.7845819399564034\n",
      "Epoch:1080 | Loss:1.2850134385079224 | Fidelity:0.7815079217949571\n",
      "Test-Epoch:1080 | Loss:1.279539419167639 | Fidelity:0.7851598680169258\n",
      "Epoch:1085 | Loss:1.2840141006342465 | Fidelity:0.7820409052659398\n",
      "Test-Epoch:1085 | Loss:1.2784967507182317 | Fidelity:0.7857227007385559\n",
      "Epoch:1090 | Loss:1.2830309051853355 | Fidelity:0.7825666409579201\n",
      "Test-Epoch:1090 | Loss:1.277473908645551 | Fidelity:0.7862754211772119\n",
      "Epoch:1095 | Loss:1.2820972383855376 | Fidelity:0.7830689029157696\n",
      "Test-Epoch:1095 | Loss:1.2765093409999275 | Fidelity:0.7867979206030176\n",
      "Epoch:1100 | Loss:1.2811361572713507 | Fidelity:0.7835844723264673\n",
      "Test-Epoch:1100 | Loss:1.2755239325343726 | Fidelity:0.7873298632378299\n",
      "Epoch:1105 | Loss:1.2801914283581195 | Fidelity:0.7840925555510077\n",
      "Test-Epoch:1105 | Loss:1.2745584032661312 | Fidelity:0.7878516781847458\n",
      "Epoch:1110 | Loss:1.2792319730232795 | Fidelity:0.7846084348886312\n",
      "Test-Epoch:1110 | Loss:1.27357890250666 | Fidelity:0.7883809489021253\n",
      "Epoch:1115 | Loss:1.2782162241927424 | Fidelity:0.7851530679667468\n",
      "Test-Epoch:1115 | Loss:1.272531331806045 | Fidelity:0.7889475038395515\n",
      "Epoch:1120 | Loss:1.277253980722715 | Fidelity:0.7856730555040187\n",
      "Test-Epoch:1120 | Loss:1.2715308280513942 | Fidelity:0.7894925332128045\n",
      "Epoch:1125 | Loss:1.2762381042242608 | Fidelity:0.7862208721058206\n",
      "Test-Epoch:1125 | Loss:1.2704690467069315 | Fidelity:0.7900710032451418\n",
      "Epoch:1130 | Loss:1.275290893531295 | Fidelity:0.7867361118825411\n",
      "Test-Epoch:1130 | Loss:1.2694805244663636 | Fidelity:0.7906126433901746\n",
      "Epoch:1135 | Loss:1.2743375993948916 | Fidelity:0.7872555532749044\n",
      "Test-Epoch:1135 | Loss:1.268485408803843 | Fidelity:0.7911587900671965\n",
      "Epoch:1140 | Loss:1.2735009472451293 | Fidelity:0.7877170282695507\n",
      "Test-Epoch:1140 | Loss:1.2676321159453576 | Fidelity:0.7916287126443534\n",
      "Epoch:1145 | Loss:1.2725587279907984 | Fidelity:0.7882321638561414\n",
      "Test-Epoch:1145 | Loss:1.2666665296289812 | Fidelity:0.792158286658142\n",
      "Epoch:1150 | Loss:1.2715637155791852 | Fidelity:0.7887756625889578\n",
      "Test-Epoch:1150 | Loss:1.2656295339292847 | Fidelity:0.7927291849962397\n",
      "Epoch:1155 | Loss:1.2705966999304872 | Fidelity:0.7893068349562036\n",
      "Test-Epoch:1155 | Loss:1.2646145638664088 | Fidelity:0.7932911476896739\n",
      "Epoch:1160 | Loss:1.2697056885670608 | Fidelity:0.7898006278852063\n",
      "Test-Epoch:1160 | Loss:1.263687912682458 | Fidelity:0.7938065016642171\n",
      "Epoch:1165 | Loss:1.2687470175098459 | Fidelity:0.7903304396750995\n",
      "Test-Epoch:1165 | Loss:1.262679998363709 | Fidelity:0.7943676505615545\n",
      "Epoch:1170 | Loss:1.2677653169033376 | Fidelity:0.7908738162633141\n",
      "Test-Epoch:1170 | Loss:1.2616394513592275 | Fidelity:0.794948897400335\n",
      "Epoch:1175 | Loss:1.2668435788013017 | Fidelity:0.791387963329431\n",
      "Test-Epoch:1175 | Loss:1.260666562931168 | Fidelity:0.7954950354186658\n",
      "Epoch:1180 | Loss:1.2659002094073057 | Fidelity:0.7919147180610306\n",
      "Test-Epoch:1180 | Loss:1.2596685282188407 | Fidelity:0.7960563080263096\n",
      "Epoch:1185 | Loss:1.2649312767136707 | Fidelity:0.7924564575939937\n",
      "Test-Epoch:1185 | Loss:1.2586351456075506 | Fidelity:0.7966393163501015\n",
      "Epoch:1190 | Loss:1.2639932530822071 | Fidelity:0.7929837681060935\n",
      "Test-Epoch:1190 | Loss:1.2576366410015605 | Fidelity:0.7972048885626914\n",
      "Epoch:1195 | Loss:1.2630208537111536 | Fidelity:0.7935308426405958\n",
      "Test-Epoch:1195 | Loss:1.2565918497205286 | Fidelity:0.7977984519779557\n",
      "Epoch:1200 | Loss:1.2620749078940097 | Fidelity:0.7940658618746973\n",
      "Test-Epoch:1200 | Loss:1.2555760579638708 | Fidelity:0.7983779568647302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1205 | Loss:1.2611712443959575 | Fidelity:0.7945800830320426\n",
      "Test-Epoch:1205 | Loss:1.2546146668023406 | Fidelity:0.7989281771310506\n",
      "Epoch:1210 | Loss:1.260214555222202 | Fidelity:0.7951239608234477\n",
      "Test-Epoch:1210 | Loss:1.253585489341858 | Fidelity:0.7995184069096883\n",
      "Epoch:1215 | Loss:1.2592684580633793 | Fidelity:0.7956639522499815\n",
      "Test-Epoch:1215 | Loss:1.252565722760144 | Fidelity:0.8001054905414711\n",
      "Epoch:1220 | Loss:1.2583263178377229 | Fidelity:0.7962035804216553\n",
      "Test-Epoch:1220 | Loss:1.2515476640248204 | Fidelity:0.8006937010057043\n",
      "Epoch:1225 | Loss:1.257364783682045 | Fidelity:0.796755427810768\n",
      "Test-Epoch:1225 | Loss:1.2505000571048033 | Fidelity:0.8013011062453964\n",
      "Epoch:1230 | Loss:1.256481054185897 | Fidelity:0.7972668821029322\n",
      "Test-Epoch:1230 | Loss:1.249556686726279 | Fidelity:0.8018498795799118\n",
      "Epoch:1235 | Loss:1.2555978980401428 | Fidelity:0.797779137295744\n",
      "Test-Epoch:1235 | Loss:1.2486175716929167 | Fidelity:0.8023971260861472\n",
      "Epoch:1240 | Loss:1.2546832635547365 | Fidelity:0.7983099187710475\n",
      "Test-Epoch:1240 | Loss:1.247634465095428 | Fidelity:0.8029716556231163\n",
      "Epoch:1245 | Loss:1.2538276816352685 | Fidelity:0.7988097412089575\n",
      "Test-Epoch:1245 | Loss:1.246730679573727 | Fidelity:0.8035013483159993\n",
      "Epoch:1250 | Loss:1.2530735960981645 | Fidelity:0.7992541770200877\n",
      "Test-Epoch:1250 | Loss:1.2459746300574215 | Fidelity:0.8039441256288417\n",
      "Epoch:1255 | Loss:1.2522930484975583 | Fidelity:0.799713600431948\n",
      "Test-Epoch:1255 | Loss:1.2451863869164241 | Fidelity:0.8044061814566849\n",
      "Epoch:1260 | Loss:1.251539015881377 | Fidelity:0.800158750546523\n",
      "Test-Epoch:1260 | Loss:1.2444353463848425 | Fidelity:0.8048468688326402\n",
      "Epoch:1265 | Loss:1.2507214210125268 | Fidelity:0.800640459613394\n",
      "Test-Epoch:1265 | Loss:1.2435880007475455 | Fidelity:0.8053467041299598\n",
      "Epoch:1270 | Loss:1.249868411654989 | Fidelity:0.8011435084368209\n",
      "Test-Epoch:1270 | Loss:1.242677099218549 | Fidelity:0.8058871385170395\n",
      "Epoch:1275 | Loss:1.2489161388770547 | Fidelity:0.8017045028873915\n",
      "Test-Epoch:1275 | Loss:1.2416061915338186 | Fidelity:0.8065270239213811\n",
      "Epoch:1280 | Loss:1.2480759372180725 | Fidelity:0.8022039000100047\n",
      "Test-Epoch:1280 | Loss:1.2406860813857838 | Fidelity:0.8070786636662829\n",
      "Epoch:1285 | Loss:1.2471583088667493 | Fidelity:0.80274893295817\n",
      "Test-Epoch:1285 | Loss:1.2396475097771256 | Fidelity:0.8077041204519672\n",
      "Epoch:1290 | Loss:1.246280251824072 | Fidelity:0.8032732457888988\n",
      "Test-Epoch:1290 | Loss:1.2386621065865424 | Fidelity:0.8082996125683712\n",
      "Epoch:1295 | Loss:1.2454286440845839 | Fidelity:0.8037839664953017\n",
      "Test-Epoch:1295 | Loss:1.2377155176037848 | Fidelity:0.8088732012101623\n",
      "Epoch:1300 | Loss:1.2446501614346197 | Fidelity:0.8042538414259459\n",
      "Test-Epoch:1300 | Loss:1.2368824888832934 | Fidelity:0.8093784634043784\n",
      "Epoch:1305 | Loss:1.2438681816015982 | Fidelity:0.80472659145354\n",
      "Test-Epoch:1305 | Loss:1.2360477838370871 | Fidelity:0.80988563486265\n",
      "Epoch:1310 | Loss:1.2430280861748735 | Fidelity:0.8052341780887987\n",
      "Test-Epoch:1310 | Loss:1.2351219136915814 | Fidelity:0.8104505504526974\n",
      "Epoch:1315 | Loss:1.2422076097358774 | Fidelity:0.8057317688788445\n",
      "Test-Epoch:1315 | Loss:1.2342196917024995 | Fidelity:0.8110028724420111\n",
      "Epoch:1320 | Loss:1.2414669070328483 | Fidelity:0.8061838270932269\n",
      "Test-Epoch:1320 | Loss:1.233439647957249 | Fidelity:0.8114807588909866\n",
      "Epoch:1325 | Loss:1.2406755417014128 | Fidelity:0.8066665858577398\n",
      "Test-Epoch:1325 | Loss:1.232577313753692 | Fidelity:0.8120112781241184\n",
      "Epoch:1330 | Loss:1.2398464737574864 | Fidelity:0.8071728938889648\n",
      "Test-Epoch:1330 | Loss:1.2316482407996274 | Fidelity:0.8125854756064206\n",
      "Epoch:1335 | Loss:1.2390858777271025 | Fidelity:0.8076400159894603\n",
      "Test-Epoch:1335 | Loss:1.2308226693261137 | Fidelity:0.8130966110178626\n"
     ]
    }
   ],
   "source": [
    "loss_hist=[]\n",
    "fid_hist=[]\n",
    "\n",
    "loss_hist_test=[]\n",
    "fid_hist_test=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batches = iterate_batches(X=training_data, batch_size=batch_size)\n",
    "    for xbatch in batches:\n",
    "        encoder_params = opt.step(cost, encoder_params, X=xbatch)\n",
    "\n",
    "        \n",
    "    if epoch%5 == 0:\n",
    "        \n",
    "        loss_training = cost(encoder_params, X_training )\n",
    "        fidel = fidelity(encoder_params, X_training )\n",
    "        \n",
    "        loss_hist.append(loss_training)\n",
    "        fid_hist.append(fidel)\n",
    "        print(\"Epoch:{} | Loss:{} | Fidelity:{}\".format(epoch, loss_training, fidel))\n",
    "\n",
    "        loss_test = cost(encoder_params, X_tes )\n",
    "        fidel = fidelity(encoder_params, X_tes )\n",
    "        loss_hist_test.append(loss_test)\n",
    "        fid_hist_test.append(fidel)\n",
    "        print(\"Test-Epoch:{} | Loss:{} | Fidelity:{}\".format(epoch, loss_test, fidel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rezults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x for x in range(0,len(loss_hist)*5,5)],np.array(fid_hist),label=\"train fid\")\n",
    "plt.plot([x for x in range(0,len(loss_hist)*5,5)],np.array(fid_hist_test),label=\"test fid\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"6-2-6:fidelity e3\",)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"fid\")\n",
    "\n",
    "print(\"fidelity:\",fid_hist[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x for x in range(0,len(loss_hist)*5,5)],np.array(loss_hist),label=\"train loss\")\n",
    "plt.plot([x for x in range(0,len(loss_hist)*5,5)],np.array(loss_hist_test),label=\"test loss\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"6-2-6:loss e3\",)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "print(\"loss:\",loss_hist[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"training_e3\"\n",
    "\n",
    "Circuit_prop={   \"shots\":shots, \"nr_trash\":nr_trash, \"nr_latent\":nr_latent, \"nr_ent\":nr_ent }\n",
    "Training_param = { \"num_samples\" : num_samples,\n",
    "                    \"batch_size\" : batch_size,\n",
    "                    \"nr_layers\": nr_layers,\n",
    "                    \"epochs\" :epochs,\n",
    "                    \"learning_rate\" : learning_rate ,\n",
    "                    \"beta1\" : beta1,\n",
    "                    \"beta2 \": beta2,\n",
    "                     \"optimizer\":\"Adam\"}\n",
    "\n",
    "performance={\"loss_hist\":loss_hist, \"fid_hist\":fid_hist,\n",
    "             \"loss_hist_test\":loss_hist_test, \"fid_hist_test\":fid_hist_test,\n",
    "             \"encoder_params\":encoder_params}\n",
    "\n",
    "experiment_data={\"Circuit_prop\":Circuit_prop,\n",
    "                \"Training_param\":Training_param,\n",
    "                \"performance:\":performance,\n",
    "                \"Name\":name}\n",
    "\n",
    "# open file for writing\n",
    "f = open(name+\".txt\",\"w\")\n",
    "f.write( str(experiment_data) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
