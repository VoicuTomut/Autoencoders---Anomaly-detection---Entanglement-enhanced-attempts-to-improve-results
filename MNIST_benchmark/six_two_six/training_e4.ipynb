{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6->2->6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomut\\anaconda3\\envs\\qhack2022\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\tomut\\anaconda3\\envs\\qhack2022\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path\n",
    "\n",
    "from qencode.initialize import setAB_amplitude, setAux, setEnt\n",
    "from qencode.encoders import e4_entangled_zoom\n",
    "from qencode.training_circuits import swap_t\n",
    "from qencode.qubits_arrangement import QubitsArrangement\n",
    "\n",
    "from qencode.utils.mnist import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data set size: 60000\n",
      "Final data set szize: 12665\n"
     ]
    }
   ],
   "source": [
    "input_data = get_dataset(img_width=8, img_height=8, train=True)\n",
    "print(\"Original data set size:\", len(input_data))\n",
    "\n",
    "# Select only the pictures with numbers 0 or 1. (jus to compare with literature)\n",
    "filtered_data = [image for image in input_data if image[1] in [0, 1]]\n",
    "input_data = filtered_data\n",
    "print(\"Final data set szize:\", len(input_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qubits: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
     ]
    }
   ],
   "source": [
    "shots = 2500\n",
    "nr_trash=4\n",
    "nr_latent=2\n",
    "nr_ent=2\n",
    "ent_state= [1 / np.sqrt(2), 0, 0, 1 / np.sqrt(2)]\n",
    "reinit_state= [0 for i in range(2**(nr_trash+nr_ent))]\n",
    "reinit_state[0]= 1 / np.sqrt(2)\n",
    "reinit_state[3]= 1 / np.sqrt(2)\n",
    "\n",
    "spec = QubitsArrangement(nr_trash, nr_latent, nr_swap=1, nr_ent=nr_ent, nr_aux=nr_trash+nr_ent)\n",
    "print(\"Qubits:\", spec.qubits)\n",
    "\n",
    "#set up the device \n",
    "dev = qml.device(\"default.qubit\", wires=spec.num_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def training_circuit_e4(init_params,encoder_params, reinit_state):\n",
    "    # Initialization\n",
    "    \n",
    "    setAB_amplitude(spec, init_params)\n",
    "    setAux(spec, reinit_state)\n",
    "    setEnt(spec, inputs=ent_state)\n",
    "\n",
    "    #encoder \n",
    "    e4_entangled_zoom(encoder_params, spec)\n",
    "        \n",
    "\n",
    "    #swap test \n",
    "    original_qubits=[*spec.trash_qubits,*spec.ent_qubits]\n",
    "    for i in spec.swap_qubits:\n",
    "        qml.Hadamard(wires=i)\n",
    "    for i in range(len(original_qubits)):\n",
    "        qml.CSWAP(wires=[*spec.swap_qubits, spec.aux_qubits[i], original_qubits[i]])\n",
    "    for i in spec.swap_qubits:\n",
    "        qml.Hadamard(wires=i)\n",
    "\n",
    "    return [qml.probs(i) for i in spec.swap_qubits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "learning_rate = 0.0003\n",
    "batch_size = 5\n",
    "num_samples = 50\n",
    "nr_layers= 2\n",
    "\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "opt = AdamOptimizer(learning_rate, beta1=beta1, beta2=beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fid_func(output):\n",
    "    # Implemented as the Fidelity Loss\n",
    "    # output[0] because we take the probability that the state after the \n",
    "    # SWAP test is ket(0), like the reference state\n",
    "    fidelity_loss = 1 / output[0]\n",
    "    return fidelity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(encoder_params, X):\n",
    "    reinit_state = [0 for _ in range(2 ** len(spec.aux_qubits))]\n",
    "    reinit_state[0] = 1.0\n",
    "    loss = 0.0\n",
    "    for x in X:\n",
    "        output = training_circuit_e4(init_params=x[0][0],\n",
    "                                     encoder_params=encoder_params,\n",
    "                                     reinit_state=reinit_state, )[0]\n",
    "        f = fid_func(output)\n",
    "        loss = loss + f\n",
    "    return loss / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fidelity(encoder_params, X):\n",
    "    reinit_state = [0 for _ in range(2 ** len(spec.aux_qubits))]\n",
    "    reinit_state[0] = 1.0\n",
    "    loss = 0.0\n",
    "    for x in X:\n",
    "        output = training_circuit_e4(init_params=x[0][0],\n",
    "                                          encoder_params=encoder_params,\n",
    "                                          reinit_state=reinit_state)[0]\n",
    "        f = output[0]\n",
    "        loss = loss + f\n",
    "    return loss / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(X, batch_size):\n",
    "    X1 = [torch.reshape(x[0], (1, 2 ** (len(spec.latent_qubits) + len(spec.trash_qubits)))) for x in X]\n",
    "    X2 = []\n",
    "    for i in range(len(X1)):\n",
    "        X2.append([X1[1], X[i][1]])\n",
    "    X = X2\n",
    "    random.shuffle(X)\n",
    "\n",
    "    batch_list = []\n",
    "    batch = []\n",
    "    for x in X:\n",
    "        if len(batch) < batch_size:\n",
    "            batch.append(x)\n",
    "\n",
    "        else:\n",
    "            batch_list.append(batch)\n",
    "            batch = []\n",
    "    if len(batch) != 0:\n",
    "        batch_list.append(batch)\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [input_data[i] for i in range(num_samples)]\n",
    "test_data = [input_data[i] for i in range(num_samples,num_samples+num_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training = [[torch.reshape(x[0], (1, 2 ** (len(spec.latent_qubits) + len(spec.trash_qubits))))] for x in training_data]\n",
    "X_tes=[[torch.reshape(x[0], (1, 2 ** (len(spec.latent_qubits) + len(spec.trash_qubits))))] for x in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize random encoder parameters\n",
    "nr_encod_qubits = len(spec.trash_qubits) + len(spec.latent_qubits)+nr_ent\n",
    "nr_par_encoder =nr_par_encoder =  15 * int(nr_encod_qubits*(nr_encod_qubits-1)/2)\n",
    "l1_params = np.random.uniform(size=(1, nr_par_encoder), requires_grad=True)\n",
    "\n",
    "\"\"\"\n",
    "                              \n",
    "nr_encod_qubits = len(spec.trash_qubits) + len(spec.latent_qubits)\n",
    "nr_par_encoder =nr_par_encoder =  15 * int(nr_encod_qubits*(nr_encod_qubits-1)/2)\n",
    "l3_params = np.random.uniform(size=(1, nr_par_encoder), requires_grad=True)\n",
    "\"\"\"\n",
    "\n",
    "encoder_params = [*l1_params]#*l3_params]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomut\\anaconda3\\envs\\qhack2022\\lib\\site-packages\\pennylane\\_grad.py:95: UserWarning: Starting with PennyLane v0.21.0, when using Autograd, inputs have to explicitly specify requires_grad=True (or the argnum argument must be passed) in order for trainable parameters to be identified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomut\\anaconda3\\envs\\qhack2022\\lib\\site-packages\\pennylane\\math\\multi_dispatch.py:63: UserWarning: Contains tensors of types {'autograd', 'torch'}; dispatch will prioritize TensorFlow and PyTorch over autograd. Consider replacing Autograd with vanilla NumPy.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 | Loss:1.9602762288732425 | Fidelity:0.5101438160322822\n",
      "Test-Epoch:0 | Loss:1.9582151593067354 | Fidelity:0.5106803036891998\n",
      "Epoch:5 | Loss:1.9306424164279874 | Fidelity:0.5180342736553676\n",
      "Test-Epoch:5 | Loss:1.9288470347057973 | Fidelity:0.5185151532023038\n",
      "Epoch:10 | Loss:1.9011678719741816 | Fidelity:0.5263164721441406\n",
      "Test-Epoch:10 | Loss:1.89938953703176 | Fidelity:0.5268086563146614\n",
      "Epoch:15 | Loss:1.8759853226788985 | Fidelity:0.5337896065065442\n",
      "Test-Epoch:15 | Loss:1.8748482404840991 | Fidelity:0.5341117051321791\n",
      "Epoch:20 | Loss:1.850867927581378 | Fidelity:0.5414972934388369\n",
      "Test-Epoch:20 | Loss:1.8509521601386618 | Fidelity:0.54147393450652\n",
      "Epoch:25 | Loss:1.8237604450796123 | Fidelity:0.5499657770539413\n",
      "Test-Epoch:25 | Loss:1.825500696129124 | Fidelity:0.5494492499795277\n",
      "Epoch:30 | Loss:1.7958745048520757 | Fidelity:0.5588149932087064\n",
      "Test-Epoch:30 | Loss:1.7995512580670276 | Fidelity:0.5576759110794286\n",
      "Epoch:35 | Loss:1.7701870781570122 | Fidelity:0.5671447397882328\n",
      "Test-Epoch:35 | Loss:1.775817701501219 | Fidelity:0.5653328071258911\n",
      "Epoch:40 | Loss:1.748990428306661 | Fidelity:0.5741948938760352\n",
      "Test-Epoch:40 | Loss:1.7563422207631674 | Fidelity:0.5717602685342082\n",
      "Epoch:45 | Loss:1.732582571635438 | Fidelity:0.5797892922782824\n",
      "Test-Epoch:45 | Loss:1.7413482598516667 | Fidelity:0.5768281900673603\n",
      "Epoch:50 | Loss:1.7199263916421565 | Fidelity:0.5841996696491354\n",
      "Test-Epoch:50 | Loss:1.7298207784777642 | Fidelity:0.5808103851184817\n",
      "Epoch:55 | Loss:1.7097147894768818 | Fidelity:0.5878222584766122\n",
      "Test-Epoch:55 | Loss:1.7204943217981068 | Fidelity:0.5840888419353826\n",
      "Epoch:60 | Loss:1.700913254090197 | Fidelity:0.5909865945749817\n",
      "Test-Epoch:60 | Loss:1.712371563485786 | Fidelity:0.5869776231940278\n",
      "Epoch:65 | Loss:1.692840125262434 | Fidelity:0.5939154563214788\n",
      "Test-Epoch:65 | Loss:1.7047979809052987 | Fidelity:0.5896877377205597\n",
      "Epoch:70 | Loss:1.6850908479162285 | Fidelity:0.5967438898312905\n",
      "Test-Epoch:70 | Loss:1.6973902827147813 | Fidelity:0.592345520353162\n",
      "Epoch:75 | Loss:1.6774555683365275 | Fidelity:0.5995440035289026\n",
      "Test-Epoch:75 | Loss:1.6899625439649213 | Fidelity:0.5950148114976751\n",
      "Epoch:80 | Loss:1.669856433967941 | Fidelity:0.6023442192531151\n",
      "Test-Epoch:80 | Loss:1.6824670429629967 | Fidelity:0.5977148915367823\n",
      "Epoch:85 | Loss:1.662301745021985 | Fidelity:0.6051433588375589\n",
      "Test-Epoch:85 | Loss:1.6749463797611255 | Fidelity:0.6004348782708003\n",
      "Epoch:90 | Loss:1.6548509325029321 | Fidelity:0.6079215934342395\n",
      "Test-Epoch:90 | Loss:1.6674943925839063 | Fidelity:0.6031456200059463\n",
      "Epoch:95 | Loss:1.647586433173024 | Fidelity:0.6106496544934864\n",
      "Test-Epoch:95 | Loss:1.660224297554869 | Fidelity:0.6058098062341448\n",
      "Epoch:100 | Loss:1.6405911581486934 | Fidelity:0.613296709398905\n",
      "Test-Epoch:100 | Loss:1.6532435704613022 | Fidelity:0.6083904559605873\n",
      "Epoch:105 | Loss:1.6339325689767101 | Fidelity:0.6158363565091115\n",
      "Test-Epoch:105 | Loss:1.6466368406460763 | Fidelity:0.610857142828488\n",
      "Epoch:110 | Loss:1.6276549113215018 | Fidelity:0.6182499241524824\n",
      "Test-Epoch:110 | Loss:1.6404584144175431 | Fidelity:0.6131890999121595\n",
      "Epoch:115 | Loss:1.6217792013427246 | Fidelity:0.6205269773489558\n",
      "Test-Epoch:115 | Loss:1.6347337237971546 | Fidelity:0.6153751940790009\n",
      "Epoch:120 | Loss:1.6163081268821577 | Fidelity:0.6226639048952025\n",
      "Test-Epoch:120 | Loss:1.6294661446962808 | Fidelity:0.617411875448823\n",
      "Epoch:125 | Loss:1.6112323955344048 | Fidelity:0.6246617953811004\n",
      "Test-Epoch:125 | Loss:1.6246449368220413 | Fidelity:0.619300564807639\n",
      "Epoch:130 | Loss:1.6065361484323364 | Fidelity:0.6265244959795551\n",
      "Test-Epoch:130 | Loss:1.620251500436347 | Fidelity:0.6210455033752796\n",
      "Epoch:135 | Loss:1.6022004812026822 | Fidelity:0.6282572671703717\n",
      "Test-Epoch:135 | Loss:1.6162630367103636 | Fidelity:0.6226524493119127\n",
      "Epoch:140 | Loss:1.5982050980765226 | Fidelity:0.6298660889702031\n",
      "Test-Epoch:140 | Loss:1.6126539766602426 | Fidelity:0.6241281524093283\n",
      "Epoch:145 | Loss:1.594528634842555 | Fidelity:0.6313574697465087\n",
      "Test-Epoch:145 | Loss:1.609396097076995 | Fidelity:0.6254803234030876\n",
      "Epoch:150 | Loss:1.5911483468380854 | Fidelity:0.6327385259249292\n",
      "Test-Epoch:150 | Loss:1.6064582732496284 | Fidelity:0.6267177761722275\n",
      "Epoch:155 | Loss:1.588039769102075 | Fidelity:0.6340171119291758\n",
      "Test-Epoch:155 | Loss:1.603806541857785 | Fidelity:0.6278504941165912\n",
      "Epoch:160 | Loss:1.585176727630765 | Fidelity:0.6352018482326189\n",
      "Test-Epoch:160 | Loss:1.6014047629532586 | Fidelity:0.6288894929445702\n",
      "Epoch:165 | Loss:1.5825318269641944 | Fidelity:0.6363019817901978\n",
      "Test-Epoch:165 | Loss:1.5992158371229388 | Fidelity:0.6298464675259666\n",
      "Epoch:170 | Loss:1.5800773387769107 | Fidelity:0.6373270884568183\n",
      "Test-Epoch:170 | Loss:1.5972032362285498 | Fidelity:0.6307332890909709\n",
      "Epoch:175 | Loss:1.5777863017740525 | Fidelity:0.6382866766158362\n",
      "Test-Epoch:175 | Loss:1.595332551033811 | Fidelity:0.6315614527215426\n",
      "Epoch:180 | Loss:1.5756336141636291 | Fidelity:0.6391897711667879\n",
      "Test-Epoch:180 | Loss:1.5935728068874087 | Fidelity:0.6323415711575764\n",
      "Epoch:185 | Loss:1.5735969353976387 | Fidelity:0.6400445509754679\n",
      "Test-Epoch:185 | Loss:1.5918973951374509 | Fidelity:0.6330829839607837\n",
      "Epoch:190 | Loss:1.5716572837661695 | Fidelity:0.6408580900322225\n",
      "Test-Epoch:190 | Loss:1.5902845674291075 | Fidelity:0.6337935162133413\n",
      "Epoch:195 | Loss:1.5697992918973467 | Fidelity:0.6416362240069927\n",
      "Test-Epoch:195 | Loss:1.5887175162196294 | Fidelity:0.6344793895809095\n",
      "Epoch:200 | Loss:1.5680111434784498 | Fidelity:0.6423835386803336\n",
      "Test-Epoch:200 | Loss:1.5871841105444406 | Fidelity:0.6351452665402813\n",
      "Epoch:205 | Loss:1.5662842536039674 | Fidelity:0.643103459537467\n",
      "Test-Epoch:205 | Loss:1.5856763761182324 | Fidelity:0.6357943967252091\n",
      "Epoch:210 | Loss:1.5646127732570638 | Fidelity:0.6437984131650099\n",
      "Test-Epoch:210 | Loss:1.5841898120228928 | Fidelity:0.6364288306720461\n",
      "Epoch:215 | Loss:1.5629930008916537 | Fidelity:0.6444700291720185\n",
      "Test-Epoch:215 | Loss:1.5827226299141608 | Fidelity:0.6370496679292706\n",
      "Epoch:220 | Loss:1.5614227764212438 | Fidelity:0.6451193538390331\n",
      "Test-Epoch:220 | Loss:1.5812749902202636 | Fidelity:0.6376573111416188\n",
      "Epoch:225 | Loss:1.5599009191540867 | Fidelity:0.6457470517365297\n",
      "Test-Epoch:225 | Loss:1.5798482952506645 | Fidelity:0.6382517037443918\n",
      "Epoch:230 | Loss:1.5584267540063723 | Fidelity:0.6463535779447304\n",
      "Test-Epoch:230 | Loss:1.578444582652349 | Fidelity:0.6388325353752005\n",
      "Epoch:235 | Loss:1.5569997517312297 | Fidelity:0.6469393104376031\n",
      "Test-Epoch:235 | Loss:1.5770660454741872 | Fidelity:0.6393994054694526\n",
      "Epoch:240 | Loss:1.5556192908431883 | Fidelity:0.6475046389839266\n",
      "Test-Epoch:240 | Loss:1.5757146886458486 | Fidelity:0.639951941364612\n",
      "Epoch:245 | Loss:1.5542845332017976 | Fidelity:0.6480500129213509\n",
      "Test-Epoch:245 | Loss:1.5743921172527267 | Fidelity:0.6404898722973655\n",
      "Epoch:250 | Loss:1.55299439330968 | Fidelity:0.6485759548065998\n",
      "Test-Epoch:250 | Loss:1.5730994405275838 | Fidelity:0.6410130647012194\n",
      "Epoch:255 | Loss:1.5517475741695068 | Fidelity:0.6490830498433822\n",
      "Test-Epoch:255 | Loss:1.571837267511539 | Fidelity:0.6415215270228161\n",
      "Epoch:260 | Loss:1.5505426402294278 | Fidelity:0.6495719220016867\n",
      "Test-Epoch:260 | Loss:1.5706057660736155 | Fidelity:0.6420153937660542\n",
      "Epoch:265 | Loss:1.5493781000624525 | Fidelity:0.6500432070053964\n",
      "Test-Epoch:265 | Loss:1.5694047563822797 | Fidelity:0.6424948986306122\n",
      "Epoch:270 | Loss:1.5482524769930253 | Fidelity:0.6504975302472473\n",
      "Test-Epoch:270 | Loss:1.568233812702089 | Fidelity:0.642960345542109\n",
      "Epoch:275 | Loss:1.5471643536137092 | Fidelity:0.6509354947048577\n",
      "Test-Epoch:275 | Loss:1.5670923529540541 | Fidelity:0.6434120843124974\n",
      "Epoch:280 | Loss:1.5461123846225768 | Fidelity:0.6513576806489486\n",
      "Test-Epoch:280 | Loss:1.5659797029241738 | Fidelity:0.6438504949791836\n",
      "Epoch:285 | Loss:1.5450952803059355 | Fidelity:0.6517646559030814\n",
      "Test-Epoch:285 | Loss:1.5648951301636378 | Fidelity:0.6442759819917877\n",
      "Epoch:290 | Loss:1.5441117691326363 | Fidelity:0.6521569930963744\n",
      "Test-Epoch:290 | Loss:1.5638378501986148 | Fidelity:0.6446889768167636\n",
      "Epoch:295 | Loss:1.5431605514927162 | Fidelity:0.6525352890663263\n",
      "Test-Epoch:295 | Loss:1.5628070134424261 | Fidelity:0.6450899456345113\n",
      "Epoch:300 | Loss:1.5422402572565164 | Fidelity:0.6529001814422621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-Epoch:300 | Loss:1.5618016842985492 | Fidelity:0.6454793979000831\n",
      "Epoch:305 | Loss:1.5413494177640705 | Fidelity:0.6532523583701956\n",
      "Test-Epoch:305 | Loss:1.560820824041623 | Fidelity:0.6458578917090003\n",
      "Epoch:310 | Loss:1.5404864588258715 | Fidelity:0.6535925590175954\n",
      "Test-Epoch:310 | Loss:1.5598632865141864 | Fidelity:0.6462260329993605\n",
      "Epoch:315 | Loss:1.5396497164307557 | Fidelity:0.6539215644762638\n",
      "Test-Epoch:315 | Loss:1.5589278314107107 | Fidelity:0.6465844672746589\n",
      "Epoch:320 | Loss:1.5388374722872877 | Fidelity:0.6542401804988163\n",
      "Test-Epoch:320 | Loss:1.5580131551789236 | Fidelity:0.6469338642934863\n",
      "Epoch:325 | Loss:1.5380480030037031 | Fidelity:0.6545492147959684\n",
      "Test-Epoch:325 | Loss:1.5571179355358313 | Fidelity:0.6472748976194436\n",
      "Epoch:330 | Loss:1.5372796351179778 | Fidelity:0.6548494522074735\n",
      "Test-Epoch:330 | Loss:1.5562408830781704 | Fidelity:0.6476082217791331\n",
      "Epoch:335 | Loss:1.5365307983055134 | Fidelity:0.6551416309567374\n",
      "Test-Epoch:335 | Loss:1.555380792733132 | Fidelity:0.6479344499575532\n",
      "Epoch:340 | Loss:1.53580007051422 | Fidelity:0.6554264225764128\n",
      "Test-Epoch:340 | Loss:1.5545365886181026 | Fidelity:0.648254134766917\n",
      "Epoch:345 | Loss:1.5350862109180534 | Fidelity:0.6557044171889115\n",
      "Test-Epoch:345 | Loss:1.5537073577188343 | Fidelity:0.6485677538648299\n",
      "Epoch:350 | Loss:1.5343881788656315 | Fidelity:0.6559761148734701\n",
      "Test-Epoch:350 | Loss:1.5528923700310207 | Fidelity:0.648875701305212\n",
      "Epoch:355 | Loss:1.5337051390031085 | Fidelity:0.6562419230214627\n",
      "Test-Epoch:355 | Loss:1.5520910849184317 | Fidelity:0.649178284674315\n",
      "Epoch:360 | Loss:1.5330364542341814 | Fidelity:0.6565021589677865\n",
      "Test-Epoch:360 | Loss:1.551303145090606 | Fidelity:0.6494757274167335\n",
      "Epoch:365 | Loss:1.5323816690811976 | Fidelity:0.656757056813014\n",
      "Test-Epoch:365 | Loss:1.5505283606617968 | Fidelity:0.6497681753404996\n",
      "Epoch:370 | Loss:1.5317404863849005 | Fidelity:0.6570067771953159\n",
      "Test-Epoch:370 | Loss:1.5497666862405484 | Fidelity:0.6500557060974966\n",
      "Epoch:375 | Loss:1.5311127402403903 | Fidelity:0.6572514187861529\n",
      "Test-Epoch:375 | Loss:1.5490181940223826 | Fidelity:0.6503383404247316\n",
      "Epoch:380 | Loss:1.5304983677432424 | Fidelity:0.6574910304161284\n",
      "Test-Epoch:380 | Loss:1.548283045555607 | Fidelity:0.6506160540500185\n",
      "Epoch:385 | Loss:1.5298973816322652 | Fidelity:0.6577256229381162\n",
      "Test-Epoch:385 | Loss:1.5475614643568745 | Fidelity:0.6508887893605523\n",
      "Epoch:390 | Loss:1.5293098453614862 | Fidelity:0.6579551801638245\n",
      "Test-Epoch:390 | Loss:1.5468537109802591 | Fidelity:0.6511564661609677\n",
      "Epoch:395 | Loss:1.5287358515856304 | Fidelity:0.6581796684376626\n",
      "Test-Epoch:395 | Loss:1.5461600615731212 | Fidelity:0.6514189910765477\n",
      "Epoch:400 | Loss:1.528175504551079 | Fidelity:0.6583990446173115\n",
      "Test-Epoch:400 | Loss:1.54548079043984 | Fidelity:0.6516762653645762\n",
      "Epoch:405 | Loss:1.5276289064771669 | Fidelity:0.6586132624016046\n",
      "Test-Epoch:405 | Loss:1.544816156711646 | Fidelity:0.6519281910696684\n",
      "Epoch:410 | Loss:1.5270961477045812 | Fidelity:0.6588222770772991\n",
      "Test-Epoch:410 | Loss:1.5441663949010265 | Fidelity:0.6521746755911701\n",
      "Epoch:415 | Loss:1.5265773001783784 | Fidelity:0.6590260488468853\n",
      "Test-Epoch:415 | Loss:1.543531708901537 | Fidelity:0.652415634822472\n"
     ]
    }
   ],
   "source": [
    "loss_hist=[]\n",
    "fid_hist=[]\n",
    "\n",
    "loss_hist_test=[]\n",
    "fid_hist_test=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batches = iterate_batches(X=training_data, batch_size=batch_size)\n",
    "    for xbatch in batches:\n",
    "        encoder_params = opt.step(cost, encoder_params, X=xbatch)\n",
    "\n",
    "\n",
    "    \n",
    "    if epoch%5 == 0:\n",
    "        \n",
    "        loss_training = cost(encoder_params, X_training )\n",
    "        fidel = fidelity(encoder_params, X_training )\n",
    "        \n",
    "        loss_hist.append(loss_training)\n",
    "        fid_hist.append(fidel)\n",
    "        print(\"Epoch:{} | Loss:{} | Fidelity:{}\".format(epoch, loss_training, fidel))\n",
    "\n",
    "        loss_test = cost(encoder_params, X_tes )\n",
    "        fidel = fidelity(encoder_params, X_tes )\n",
    "        loss_hist_test.append(loss_test)\n",
    "        fid_hist_test.append(fidel)\n",
    "        print(\"Test-Epoch:{} | Loss:{} | Fidelity:{}\".format(epoch, loss_test, fidel))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rezults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x for x in range(0,len(loss_hist)*5,5)],np.array(fid_hist),label=\"train fid\")\n",
    "plt.plot([x for x in range(0,len(loss_hist)*5,5)],np.array(fid_hist_test),label=\"test fid\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"6-2-6:fidelity e4\",)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"fid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x for x in range(0,len(loss_hist)*5,5)],np.array(loss_hist),label=\"train loss\")\n",
    "plt.plot([x for x in range(0,len(loss_hist)*5,5)],np.array(loss_hist_test),label=\"test loss\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"6-2-6:loss e4\",)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"training_e4\"\n",
    "\n",
    "Circuit_prop={   \"shots\":shots, \"nr_trash\":nr_trash, \"nr_latent\":nr_latent ,\"nr_ent\":nr_ent ,\"ent_state\":ent_state }\n",
    "Training_param = { \"num_samples\" : num_samples,\n",
    "                    \"batch_size\" :batch_size,\n",
    "                    \"nr_layers\":nr_layers,\n",
    "                    \"epochs\" :epochs,\n",
    "                    \"learning_rate\" : learning_rate ,\n",
    "                    \"beta1\" : beta1,\n",
    "                    \"beta2 \":beta2,\n",
    "                     \"optimizer\":\"Adam\"}\n",
    "\n",
    "\n",
    "performance={\"loss_hist\":loss_hist, \"fid_hist\":fid_hist,\n",
    "             \"loss_hist_test\":loss_hist_test, \"fid_hist_test\":fid_hist_test,\n",
    "             \"encoder_params\":encoder_params}\n",
    "\n",
    "\n",
    "\n",
    "# open file for writing\n",
    "f = open(name+\".txt\",\"w\")\n",
    "f.write( str(experiment_data) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
