{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6->1->6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomut\\anaconda3\\envs\\qhack2022\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\tomut\\anaconda3\\envs\\qhack2022\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path\n",
    "\n",
    "from qencode.initialize import setAB_amplitude, setAux, setEnt\n",
    "from qencode.encoders import e4_entangled_zoom\n",
    "from qencode.training_circuits import swap_t\n",
    "from qencode.qubits_arrangement import QubitsArrangement\n",
    "\n",
    "from qencode.utils.mnist import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data set size: 60000\n",
      "Final data set szize: 12665\n"
     ]
    }
   ],
   "source": [
    "input_data = get_dataset(img_width=8, img_height=8, train=True)\n",
    "print(\"Original data set size:\", len(input_data))\n",
    "\n",
    "# Select only the pictures with numbers 0 or 1. (jus to compare with literature)\n",
    "filtered_data = [image for image in input_data if image[1] in [0, 1]]\n",
    "input_data = filtered_data\n",
    "print(\"Final data set szize:\", len(input_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qubits: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "shots = 2500\n",
    "nr_trash=5\n",
    "nr_latent=1\n",
    "nr_ent=2\n",
    "ent_state= [1 / np.sqrt(2), 0, 0, 1 / np.sqrt(2)]\n",
    "reinit_state= [0 for i in range(2**(nr_trash+nr_ent))]\n",
    "reinit_state[0]= 1 / np.sqrt(2)\n",
    "reinit_state[3]= 1 / np.sqrt(2)\n",
    "\n",
    "spec = QubitsArrangement(nr_trash, nr_latent, nr_swap=1, nr_ent=nr_ent, nr_aux=nr_trash+nr_ent)\n",
    "print(\"Qubits:\", spec.qubits)\n",
    "\n",
    "#set up the device \n",
    "dev = qml.device(\"default.qubit\", wires=spec.num_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def training_circuit_e4(init_params,encoder_params, reinit_state):\n",
    "    # Initialization\n",
    "    \n",
    "    setAB_amplitude(spec, init_params)\n",
    "    setAux(spec, reinit_state)\n",
    "    setEnt(spec, inputs=ent_state)\n",
    "\n",
    "    #encoder \n",
    "    e4_entangled_zoom(encoder_params, spec)\n",
    "        \n",
    "\n",
    "    #swap test \n",
    "    original_qubits=[*spec.trash_qubits,*spec.ent_qubits]\n",
    "    for i in spec.swap_qubits:\n",
    "        qml.Hadamard(wires=i)\n",
    "    for i in range(len(original_qubits)):\n",
    "        qml.CSWAP(wires=[*spec.swap_qubits, spec.aux_qubits[i], original_qubits[i]])\n",
    "    for i in spec.swap_qubits:\n",
    "        qml.Hadamard(wires=i)\n",
    "\n",
    "    return [qml.probs(i) for i in spec.swap_qubits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "learning_rate = 0.0003\n",
    "batch_size = 5\n",
    "num_samples = 50\n",
    "nr_layers= 2\n",
    "\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "opt = AdamOptimizer(learning_rate, beta1=beta1, beta2=beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fid_func(output):\n",
    "    # Implemented as the Fidelity Loss\n",
    "    # output[0] because we take the probability that the state after the \n",
    "    # SWAP test is ket(0), like the reference state\n",
    "    fidelity_loss = 1 / output[0]\n",
    "    return fidelity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(encoder_params, X):\n",
    "    reinit_state = [0 for _ in range(2 ** len(spec.aux_qubits))]\n",
    "    reinit_state[0] = 1.0\n",
    "    loss = 0.0\n",
    "    for x in X:\n",
    "        output = training_circuit_e4(init_params=x[0][0],\n",
    "                                     encoder_params=encoder_params,\n",
    "                                     reinit_state=reinit_state, )[0]\n",
    "        f = fid_func(output)\n",
    "        loss = loss + f\n",
    "    return loss / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fidelity(encoder_params, X):\n",
    "    reinit_state = [0 for _ in range(2 ** len(spec.aux_qubits))]\n",
    "    reinit_state[0] = 1.0\n",
    "    loss = 0.0\n",
    "    for x in X:\n",
    "        output = training_circuit_e4(init_params=x[0][0],\n",
    "                                          encoder_params=encoder_params,\n",
    "                                          reinit_state=reinit_state)[0]\n",
    "        f = output[0]\n",
    "        loss = loss + f\n",
    "    return loss / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(X, batch_size):\n",
    "    X1 = [torch.reshape(x[0], (1, 2 ** (len(spec.latent_qubits) + len(spec.trash_qubits)))) for x in X]\n",
    "    X2 = []\n",
    "    for i in range(len(X1)):\n",
    "        X2.append([X1[1], X[i][1]])\n",
    "    X = X2\n",
    "    random.shuffle(X)\n",
    "\n",
    "    batch_list = []\n",
    "    batch = []\n",
    "    for x in X:\n",
    "        if len(batch) < batch_size:\n",
    "            batch.append(x)\n",
    "\n",
    "        else:\n",
    "            batch_list.append(batch)\n",
    "            batch = []\n",
    "    if len(batch) != 0:\n",
    "        batch_list.append(batch)\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [input_data[i] for i in range(num_samples)]\n",
    "test_data = [input_data[i] for i in range(num_samples,num_samples+num_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training = [[torch.reshape(x[0], (1, 2 ** (len(spec.latent_qubits) + len(spec.trash_qubits))))] for x in training_data]\n",
    "X_tes=[[torch.reshape(x[0], (1, 2 ** (len(spec.latent_qubits) + len(spec.trash_qubits))))] for x in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize random encoder parameters\n",
    "nr_encod_qubits = len(spec.trash_qubits) + len(spec.latent_qubits)+nr_ent\n",
    "nr_par_encoder =nr_par_encoder =  15 * int(nr_encod_qubits*(nr_encod_qubits-1)/2)\n",
    "l1_params = np.random.uniform(size=(1, nr_par_encoder), requires_grad=True)\n",
    "\n",
    "\n",
    "                              \n",
    "\n",
    "\"\"\"\n",
    "                              \n",
    "nr_encod_qubits = len(spec.trash_qubits) + len(spec.latent_qubits)\n",
    "nr_par_encoder =nr_par_encoder =  15 * int(nr_encod_qubits*(nr_encod_qubits-1)/2)\n",
    "l3_params = np.random.uniform(size=(1, nr_par_encoder), requires_grad=True)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "encoder_params = [*l1_params]#*l3_params]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomut\\anaconda3\\envs\\qhack2022\\lib\\site-packages\\pennylane\\_grad.py:95: UserWarning: Starting with PennyLane v0.21.0, when using Autograd, inputs have to explicitly specify requires_grad=True (or the argnum argument must be passed) in order for trainable parameters to be identified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomut\\anaconda3\\envs\\qhack2022\\lib\\site-packages\\pennylane\\math\\multi_dispatch.py:63: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow and PyTorch over autograd. Consider replacing Autograd with vanilla NumPy.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 | Loss:1.9585827084284435 | Fidelity:0.5107077003586942\n",
      "Test-Epoch:0 | Loss:1.9667798513990562 | Fidelity:0.5086409420085658\n",
      "Epoch:5 | Loss:1.9278789482962209 | Fidelity:0.518942086741219\n",
      "Test-Epoch:5 | Loss:1.9351860348431487 | Fidelity:0.5170629286577814\n",
      "Epoch:10 | Loss:1.8982117793258653 | Fidelity:0.5271951153770629\n",
      "Test-Epoch:10 | Loss:1.904867404338317 | Fidelity:0.5254388599955924\n",
      "Epoch:15 | Loss:1.8691959565007161 | Fidelity:0.5356002104779766\n",
      "Test-Epoch:15 | Loss:1.8748417339892387 | Fidelity:0.534065141324863\n",
      "Epoch:20 | Loss:1.843005193774791 | Fidelity:0.5435093099919549\n",
      "Test-Epoch:20 | Loss:1.8471447905361404 | Fidelity:0.542333554240021\n",
      "Epoch:25 | Loss:1.8191785961895703 | Fidelity:0.5509550150341676\n",
      "Test-Epoch:25 | Loss:1.8221159312752837 | Fidelity:0.5500582640608151\n",
      "Epoch:30 | Loss:1.797782377013073 | Fidelity:0.5577978003415708\n",
      "Test-Epoch:30 | Loss:1.8001796274857491 | Fidelity:0.5570006813426686\n",
      "Epoch:35 | Loss:1.7793528197203665 | Fidelity:0.5638039538618953\n",
      "Test-Epoch:35 | Loss:1.7815993720865362 | Fidelity:0.5630038977042134\n",
      "Epoch:40 | Loss:1.7632085246368436 | Fidelity:0.5691461566063125\n",
      "Test-Epoch:40 | Loss:1.7655002071880728 | Fidelity:0.5682933520190241\n",
      "Epoch:45 | Loss:1.7484170544431314 | Fidelity:0.5741020987837014\n",
      "Test-Epoch:45 | Loss:1.7508841803012694 | Fidelity:0.5731639759963258\n",
      "Epoch:50 | Loss:1.7344219771376024 | Fidelity:0.5788473737029363\n",
      "Test-Epoch:50 | Loss:1.7371740063046863 | Fidelity:0.5777955708172695\n",
      "Epoch:55 | Loss:1.7210751852522035 | Fidelity:0.5834317623667694\n",
      "Test-Epoch:55 | Loss:1.7241977164132454 | Fidelity:0.5822424384842848\n",
      "Epoch:60 | Loss:1.7084075911656145 | Fidelity:0.5878446924248325\n",
      "Test-Epoch:60 | Loss:1.7119554584092123 | Fidelity:0.586500501875859\n",
      "Epoch:65 | Loss:1.696452746830232 | Fidelity:0.5920701114475081\n",
      "Test-Epoch:65 | Loss:1.7004504309007282 | Fidelity:0.5905606158486439\n",
      "Epoch:70 | Loss:1.6851850601277045 | Fidelity:0.5961080992492089\n",
      "Test-Epoch:70 | Loss:1.689629048919329 | Fidelity:0.5944301690166074\n",
      "Epoch:75 | Loss:1.6745398994581735 | Fidelity:0.5999709421997581\n",
      "Test-Epoch:75 | Loss:1.679400706223309 | Fidelity:0.5981295877392833\n",
      "Epoch:80 | Loss:1.6644656737975705 | Fidelity:0.6036678102406962\n",
      "Test-Epoch:80 | Loss:1.6696934245842359 | Fidelity:0.6016756556345335\n",
      "Epoch:85 | Loss:1.6549527398984956 | Fidelity:0.6071953537913484\n",
      "Test-Epoch:85 | Loss:1.6604866386691903 | Fidelity:0.6050701633582696\n",
      "Epoch:90 | Loss:1.646020165687196 | Fidelity:0.6105413718541016\n",
      "Test-Epoch:90 | Loss:1.651797929341024 | Fidelity:0.6083028832367904\n",
      "Epoch:95 | Loss:1.6376890156159087 | Fidelity:0.6136931681850226\n",
      "Test-Epoch:95 | Loss:1.643653570529313 | Fidelity:0.6113605159222718\n",
      "Epoch:100 | Loss:1.6299694825432767 | Fidelity:0.6166417342263112\n",
      "Test-Epoch:100 | Loss:1.636072582293031 | Fidelity:0.6142319807964519\n",
      "Epoch:105 | Loss:1.6228620509507379 | Fidelity:0.6193814869212563\n",
      "Test-Epoch:105 | Loss:1.6290656006171609 | Fidelity:0.6169091394141953\n",
      "Epoch:110 | Loss:1.6163629863439484 | Fidelity:0.6219086406935938\n",
      "Test-Epoch:110 | Loss:1.622638641954954 | Fidelity:0.6193858407426596\n",
      "Epoch:115 | Loss:1.6104685381410644 | Fidelity:0.6242199893432323\n",
      "Test-Epoch:115 | Loss:1.6167955072768114 | Fidelity:0.6216572076761606\n",
      "Epoch:120 | Loss:1.6051758850273985 | Fidelity:0.6263127105368106\n",
      "Test-Epoch:120 | Loss:1.6115371211841072 | Fidelity:0.6237198078720192\n",
      "Epoch:125 | Loss:1.6004806983689612 | Fidelity:0.6281852466830966\n",
      "Test-Epoch:125 | Loss:1.6068585648794755 | Fidelity:0.6255726124060843\n",
      "Epoch:130 | Loss:1.596372822107657 | Fidelity:0.6298387886489194\n",
      "Test-Epoch:130 | Loss:1.6027456302957555 | Fidelity:0.6272181978316078\n",
      "Epoch:135 | Loss:1.5928321750171153 | Fidelity:0.6312786713225371\n",
      "Test-Epoch:135 | Loss:1.59917256447281 | Fidelity:0.6286635973995947\n",
      "Epoch:140 | Loss:1.5898263940294268 | Fidelity:0.6325151710909863\n",
      "Test-Epoch:140 | Loss:1.5961018576390331 | Fidelity:0.6299204570944167\n",
      "Epoch:145 | Loss:1.5873109358231792 | Fidelity:0.6335634481733023\n",
      "Test-Epoch:145 | Loss:1.593486219462148 | Fidelity:0.6310044080973125\n",
      "Epoch:150 | Loss:1.585231786195329 | Fidelity:0.6344425558233826\n",
      "Test-Epoch:150 | Loss:1.5912724405655352 | Fidelity:0.6319337336182119\n",
      "Epoch:155 | Loss:1.5835302882398397 | Fidelity:0.6351736641458016\n",
      "Test-Epoch:155 | Loss:1.5894063895047301 | Fidelity:0.6327275750018455\n",
      "Epoch:160 | Loss:1.5821488345707033 | Fidelity:0.6357779396846103\n",
      "Test-Epoch:160 | Loss:1.5878379513834888 | Fidelity:0.6334040989791672\n",
      "Epoch:165 | Loss:1.581035798175306 | Fidelity:0.6362746788668837\n",
      "Test-Epoch:165 | Loss:1.5865246541278526 | Fidelity:0.6339790929472565\n",
      "Epoch:170 | Loss:1.5801485240367201 | Fidelity:0.6366801410655731\n",
      "Test-Epoch:170 | Loss:1.585433285446473 | Fidelity:0.634465267169399\n",
      "Epoch:175 | Loss:1.579454182913976 | Fidelity:0.637007164847257\n",
      "Test-Epoch:175 | Loss:1.584539660547375 | Fidelity:0.6348722285261655\n",
      "Epoch:180 | Loss:1.578929079633249 | Fidelity:0.637265352426619\n",
      "Test-Epoch:180 | Loss:1.5838272682273153 | Fidelity:0.6352068725460922\n",
      "Epoch:185 | Loss:1.5785572002308554 | Fidelity:0.6374615338595419\n",
      "Test-Epoch:185 | Loss:1.5832855369029861 | Fidelity:0.6354739275831566\n",
      "Epoch:190 | Loss:1.578328543779184 | Fidelity:0.6376003131186845\n",
      "Test-Epoch:190 | Loss:1.582908148628103 | Fidelity:0.6356764980223774\n",
      "Epoch:195 | Loss:1.5782374917286335 | Fidelity:0.6376846092336623\n",
      "Test-Epoch:195 | Loss:1.5826915392112642 | Fidelity:0.6358165596110702\n",
      "Epoch:200 | Loss:1.5782813092027022 | Fidelity:0.6377161639291985\n",
      "Test-Epoch:200 | Loss:1.5826335979014359 | Fidelity:0.6358954040546728\n",
      "Epoch:205 | Loss:1.5784588264786747 | Fidelity:0.6376960014232784\n",
      "Test-Epoch:205 | Loss:1.5827325690851473 | Fidelity:0.6359140312337173\n",
      "Epoch:210 | Loss:1.5787693404030725 | Fidelity:0.6376248258224464\n",
      "Test-Epoch:210 | Loss:1.582986177757096 | Fidelity:0.6358734781326453\n",
      "Epoch:215 | Loss:1.5792117650111075 | Fidelity:0.6375033429649462\n",
      "Test-Epoch:215 | Loss:1.583391008395574 | Fidelity:0.6357750698890076\n",
      "Epoch:220 | Loss:1.5797840386927477 | Fidelity:0.6373325004064399\n",
      "Test-Epoch:220 | Loss:1.5839421511934682 | Fidelity:0.6356205841992777\n",
      "Epoch:225 | Loss:1.5804827635821832 | Fidelity:0.6371136505000115\n",
      "Test-Epoch:225 | Loss:1.584633091284631 | Fidelity:0.6354123344032747\n",
      "Epoch:230 | Loss:1.5813030196471498 | Fidelity:0.6368486536242034\n",
      "Test-Epoch:230 | Loss:1.5854557689916537 | Fidelity:0.6351531934498799\n",
      "Epoch:235 | Loss:1.5822382755450348 | Fidelity:0.6365399459544016\n",
      "Test-Epoch:235 | Loss:1.5864007064063887 | Fidelity:0.6348465918333115\n",
      "Epoch:240 | Loss:1.5832803249915137 | Fidelity:0.6361905935371093\n",
      "Test-Epoch:240 | Loss:1.5874571007421716 | Fidelity:0.6344965197433029\n",
      "Epoch:245 | Loss:1.584419214969255 | Fidelity:0.6358043407831601\n",
      "Test-Epoch:245 | Loss:1.588612833664235 | Fidelity:0.6341075458684593\n",
      "Epoch:250 | Loss:1.585643187462769 | Fidelity:0.6353856421356595\n",
      "Test-Epoch:250 | Loss:1.5898544199601181 | Fidelity:0.6336848398735292\n",
      "Epoch:255 | Loss:1.586938703552895 | Fidelity:0.6349396503622112\n",
      "Test-Epoch:255 | Loss:1.591166984248651 | Fidelity:0.6332341650923607\n",
      "Epoch:260 | Loss:1.5882906321897778 | Fidelity:0.6344721327239852\n",
      "Test-Epoch:260 | Loss:1.5925343787101323 | Fidelity:0.6327618034706913\n",
      "Epoch:265 | Loss:1.5896826554498733 | Fidelity:0.6339893001636759\n",
      "Test-Epoch:265 | Loss:1.593939524548853 | Fidelity:0.632274389793938\n",
      "Epoch:270 | Loss:1.591097879025799 | Fidelity:0.6334975594138491\n",
      "Test-Epoch:270 | Loss:1.595364987107687 | Fidelity:0.6317786608300263\n",
      "Epoch:275 | Loss:1.5925195689079978 | Fidelity:0.6330032226776434\n",
      "Test-Epoch:275 | Loss:1.5967937108984551 | Fidelity:0.631281154755958\n",
      "Epoch:280 | Loss:1.5939318923932664 | Fidelity:0.6325122234471275\n",
      "Test-Epoch:280 | Loss:1.5982097821454782 | Fidelity:0.630787914329778\n",
      "Epoch:285 | Loss:1.5953205403274595 | Fidelity:0.6320298847939586\n",
      "Test-Epoch:285 | Loss:1.599599075638481 | Fidelity:0.6303042467874964\n"
     ]
    }
   ],
   "source": [
    "loss_hist=[]\n",
    "fid_hist=[]\n",
    "\n",
    "loss_hist_test=[]\n",
    "fid_hist_test=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batches = iterate_batches(X=training_data, batch_size=batch_size)\n",
    "    for xbatch in batches:\n",
    "        encoder_params = opt.step(cost, encoder_params, X=xbatch)\n",
    "\n",
    "\n",
    "    \n",
    "    if epoch%5 == 0:\n",
    "        \n",
    "        loss_training = cost(encoder_params, X_training )\n",
    "        fidel = fidelity(encoder_params, X_training )\n",
    "        \n",
    "        loss_hist.append(loss_training)\n",
    "        fid_hist.append(fidel)\n",
    "        print(\"Epoch:{} | Loss:{} | Fidelity:{}\".format(epoch, loss_training, fidel))\n",
    "\n",
    "        loss_test = cost(encoder_params, X_tes )\n",
    "        fidel = fidelity(encoder_params, X_tes )\n",
    "        loss_hist_test.append(loss_test)\n",
    "        fid_hist_test.append(fidel)\n",
    "        print(\"Test-Epoch:{} | Loss:{} | Fidelity:{}\".format(epoch, loss_test, fidel))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rezults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x for x in range(0,len(loss_hist)*5,5)],np.array(fid_hist),label=\"train fid\")\n",
    "plt.plot([x for x in range(0,len(loss_hist)*5,5)],np.array(fid_hist_test),label=\"test fid\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"6-2-6:fidelity e4\",)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"fid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x for x in range(0,len(loss_hist)*5,5)],np.array(loss_hist),label=\"train loss\")\n",
    "plt.plot([x for x in range(0,len(loss_hist)*5,5)],np.array(loss_hist_test),label=\"test loss\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"6-2-6:loss e4\",)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"training_e4\"\n",
    "\n",
    "Circuit_prop={   \"shots\":shots, \"nr_trash\":nr_trash, \"nr_latent\":nr_latent ,\"nr_ent\":nr_ent ,\"ent_state\":ent_state }\n",
    "Training_param = { \"num_samples\" : num_samples,\n",
    "                    \"batch_size\" :batch_size,\n",
    "                    \"nr_layers\":nr_layers,\n",
    "                    \"epochs\" :epochs,\n",
    "                    \"learning_rate\" : learning_rate ,\n",
    "                    \"beta1\" : beta1,\n",
    "                    \"beta2 \":beta2,\n",
    "                     \"optimizer\":\"Adam\"}\n",
    "\n",
    "performance={\"loss_hist\":loss_hist, \"fid_hist\":fid_hist,\n",
    "             \"loss_hist_test\":loss_hist_test, \"fid_hist_test\":fid_hist_test,\n",
    "             \"encoder_params\":encoder_params}\n",
    "\n",
    "experiment_data={\"Circuit_prop\":Circuit_prop,\n",
    "                \"Training_param\":Training_param,\n",
    "                \"performance:\":performance,\n",
    "                \"Name\":name}\n",
    "\n",
    "# open file for writing\n",
    "f = open(name+\".txt\",\"w\")\n",
    "f.write( str(experiment_data) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
