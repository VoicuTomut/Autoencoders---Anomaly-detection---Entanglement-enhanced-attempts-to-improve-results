{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6->3->6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomut\\anaconda3\\envs\\qhack2022\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\tomut\\anaconda3\\envs\\qhack2022\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path\n",
    "\n",
    "from qencode.initialize import setAB_amplitude, setAux, setEnt\n",
    "from qencode.encoders import e4_entangled_zoom\n",
    "from qencode.training_circuits import swap_t\n",
    "from qencode.qubits_arrangement import QubitsArrangement\n",
    "\n",
    "from qencode.utils.mnist import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data set size: 60000\n",
      "Final data set szize: 12665\n"
     ]
    }
   ],
   "source": [
    "input_data = get_dataset(img_width=8, img_height=8, train=True)\n",
    "print(\"Original data set size:\", len(input_data))\n",
    "\n",
    "# Select only the pictures with numbers 0 or 1. (jus to compare with literature)\n",
    "filtered_data = [image for image in input_data if image[1] in [0, 1]]\n",
    "input_data = filtered_data\n",
    "print(\"Final data set szize:\", len(input_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qubits: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "shots = 2500\n",
    "nr_trash=3\n",
    "nr_latent=3\n",
    "nr_ent=2\n",
    "ent_state= [1 / np.sqrt(2), 0, 0, 1 / np.sqrt(2)]\n",
    "reinit_state= [0 for i in range(2**(nr_trash+nr_ent))]\n",
    "reinit_state[0]= 1 / np.sqrt(2)\n",
    "reinit_state[3]= 1 / np.sqrt(2)\n",
    "\n",
    "spec = QubitsArrangement(nr_trash, nr_latent, nr_swap=1, nr_ent=nr_ent, nr_aux=nr_trash+nr_ent)\n",
    "print(\"Qubits:\", spec.qubits)\n",
    "\n",
    "#set up the device \n",
    "dev = qml.device(\"default.qubit\", wires=spec.num_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def training_circuit_e4(init_params,encoder_params, reinit_state):\n",
    "    # Initialization\n",
    "    \n",
    "    setAB_amplitude(spec, init_params)\n",
    "    setAux(spec, reinit_state)\n",
    "    setEnt(spec, inputs=ent_state)\n",
    "\n",
    "    #encoder \n",
    "    e4_entangled_zoom(encoder_params, spec)\n",
    "        \n",
    "\n",
    "    #swap test \n",
    "    original_qubits=[*spec.trash_qubits,*spec.ent_qubits]\n",
    "    for i in spec.swap_qubits:\n",
    "        qml.Hadamard(wires=i)\n",
    "    for i in range(len(original_qubits)):\n",
    "        qml.CSWAP(wires=[*spec.swap_qubits, spec.aux_qubits[i], original_qubits[i]])\n",
    "    for i in spec.swap_qubits:\n",
    "        qml.Hadamard(wires=i)\n",
    "\n",
    "    return [qml.probs(i) for i in spec.swap_qubits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "learning_rate = 0.0003\n",
    "batch_size = 5\n",
    "num_samples = 50\n",
    "nr_layers= 2\n",
    "\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "opt = AdamOptimizer(learning_rate, beta1=beta1, beta2=beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fid_func(output):\n",
    "    # Implemented as the Fidelity Loss\n",
    "    # output[0] because we take the probability that the state after the \n",
    "    # SWAP test is ket(0), like the reference state\n",
    "    fidelity_loss = 1 / output[0]\n",
    "    return fidelity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(encoder_params, X):\n",
    "    reinit_state = [0 for _ in range(2 ** len(spec.aux_qubits))]\n",
    "    reinit_state[0] = 1.0\n",
    "    loss = 0.0\n",
    "    for x in X:\n",
    "        output = training_circuit_e4(init_params=x[0][0],\n",
    "                                     encoder_params=encoder_params,\n",
    "                                     reinit_state=reinit_state, )[0]\n",
    "        f = fid_func(output)\n",
    "        loss = loss + f\n",
    "    return loss / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fidelity(encoder_params, X):\n",
    "    reinit_state = [0 for _ in range(2 ** len(spec.aux_qubits))]\n",
    "    reinit_state[0] = 1.0\n",
    "    loss = 0.0\n",
    "    for x in X:\n",
    "        output = training_circuit_e4(init_params=x[0][0],\n",
    "                                          encoder_params=encoder_params,\n",
    "                                          reinit_state=reinit_state)[0]\n",
    "        f = output[0]\n",
    "        loss = loss + f\n",
    "    return loss / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(X, batch_size):\n",
    "    X1 = [torch.reshape(x[0], (1, 2 ** (len(spec.latent_qubits) + len(spec.trash_qubits)))) for x in X]\n",
    "    X2 = []\n",
    "    for i in range(len(X1)):\n",
    "        X2.append([X1[1], X[i][1]])\n",
    "    X = X2\n",
    "    random.shuffle(X)\n",
    "\n",
    "    batch_list = []\n",
    "    batch = []\n",
    "    for x in X:\n",
    "        if len(batch) < batch_size:\n",
    "            batch.append(x)\n",
    "\n",
    "        else:\n",
    "            batch_list.append(batch)\n",
    "            batch = []\n",
    "    if len(batch) != 0:\n",
    "        batch_list.append(batch)\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [input_data[i] for i in range(num_samples)]\n",
    "test_data = [input_data[i] for i in range(num_samples,num_samples+num_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training = [[torch.reshape(x[0], (1, 2 ** (len(spec.latent_qubits) + len(spec.trash_qubits))))] for x in training_data]\n",
    "X_tes=[[torch.reshape(x[0], (1, 2 ** (len(spec.latent_qubits) + len(spec.trash_qubits))))] for x in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize random encoder parameters\n",
    "nr_encod_qubits = len(spec.trash_qubits) + len(spec.latent_qubits)+nr_ent\n",
    "nr_par_encoder =nr_par_encoder =  15 * int(nr_encod_qubits*(nr_encod_qubits-1)/2)\n",
    "l1_params = np.random.uniform(size=(1, nr_par_encoder), requires_grad=True)\n",
    "\n",
    "\n",
    "                              \n",
    "\n",
    "\"\"\"\n",
    "                              \n",
    "nr_encod_qubits = len(spec.trash_qubits) + len(spec.latent_qubits)\n",
    "nr_par_encoder =nr_par_encoder =  15 * int(nr_encod_qubits*(nr_encod_qubits-1)/2)\n",
    "l3_params = np.random.uniform(size=(1, nr_par_encoder), requires_grad=True)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "encoder_params = [*l1_params]#*l3_params]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomut\\anaconda3\\envs\\qhack2022\\lib\\site-packages\\pennylane\\_grad.py:95: UserWarning: Starting with PennyLane v0.21.0, when using Autograd, inputs have to explicitly specify requires_grad=True (or the argnum argument must be passed) in order for trainable parameters to be identified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomut\\anaconda3\\envs\\qhack2022\\lib\\site-packages\\pennylane\\math\\multi_dispatch.py:63: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow and PyTorch over autograd. Consider replacing Autograd with vanilla NumPy.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 | Loss:1.8765964420665153 | Fidelity:0.5329491458023166\n",
      "Test-Epoch:0 | Loss:1.8763537550311002 | Fidelity:0.532999625525941\n",
      "Epoch:5 | Loss:1.8246148696343816 | Fidelity:0.5481330760641479\n",
      "Test-Epoch:5 | Loss:1.8269728471495028 | Fidelity:0.5474486196792413\n",
      "Epoch:10 | Loss:1.7878848235252798 | Fidelity:0.5596020248598557\n",
      "Test-Epoch:10 | Loss:1.7905571204181603 | Fidelity:0.5588370934102334\n",
      "Epoch:15 | Loss:1.7560248680447357 | Fidelity:0.5700667030519401\n",
      "Test-Epoch:15 | Loss:1.7596278389368059 | Fidelity:0.5690209354819189\n",
      "Epoch:20 | Loss:1.7262128605381124 | Fidelity:0.5802490099280243\n",
      "Test-Epoch:20 | Loss:1.73133980151999 | Fidelity:0.5786979459702243\n",
      "Epoch:25 | Loss:1.6995325325791588 | Fidelity:0.5897133871356565\n",
      "Test-Epoch:25 | Loss:1.7061485097184224 | Fidelity:0.5876340838176055\n",
      "Epoch:30 | Loss:1.678265497436814 | Fidelity:0.5975422594816993\n",
      "Test-Epoch:30 | Loss:1.6861458935880638 | Fidelity:0.5949883193107912\n",
      "Epoch:35 | Loss:1.663098882946183 | Fidelity:0.6033265432455754\n",
      "Test-Epoch:35 | Loss:1.672065234826324 | Fidelity:0.6003515227591159\n",
      "Epoch:40 | Loss:1.6529904953713705 | Fidelity:0.6073185203070838\n",
      "Test-Epoch:40 | Loss:1.6628932647872088 | Fidelity:0.6039763395698909\n",
      "Epoch:45 | Loss:1.6463094201167434 | Fidelity:0.6100494155343966\n",
      "Test-Epoch:45 | Loss:1.6569818521698938 | Fidelity:0.60640339768935\n",
      "Epoch:50 | Loss:1.641587459727309 | Fidelity:0.6120332605258042\n",
      "Test-Epoch:50 | Loss:1.6528633571035265 | Fidelity:0.6081461534435301\n",
      "Epoch:55 | Loss:1.6378297078325934 | Fidelity:0.6136295760569465\n",
      "Test-Epoch:55 | Loss:1.649574051492364 | Fidelity:0.6095515676087121\n",
      "Epoch:60 | Loss:1.634502294646625 | Fidelity:0.6150363278604\n",
      "Test-Epoch:60 | Loss:1.6466192378489937 | Fidelity:0.6108019371885118\n",
      "Epoch:65 | Loss:1.6313648842310033 | Fidelity:0.616346537452349\n",
      "Test-Epoch:65 | Loss:1.6437857173593715 | Fidelity:0.6119790971616541\n",
      "Epoch:70 | Loss:1.6283123829676491 | Fidelity:0.6176040811136358\n",
      "Test-Epoch:70 | Loss:1.6409801139822346 | Fidelity:0.613121830203502\n",
      "Epoch:75 | Loss:1.6252952562787615 | Fidelity:0.6188320344638638\n",
      "Test-Epoch:75 | Loss:1.638154660399414 | Fidelity:0.6142524039660926\n",
      "Epoch:80 | Loss:1.6222932035630009 | Fidelity:0.6200420665407385\n",
      "Test-Epoch:80 | Loss:1.6352872709732658 | Fidelity:0.615383535286332\n",
      "Epoch:85 | Loss:1.6193058291439284 | Fidelity:0.6212377663194788\n",
      "Test-Epoch:85 | Loss:1.6323764397114813 | Fidelity:0.6165199449128759\n",
      "Epoch:90 | Loss:1.6163453482099097 | Fidelity:0.6224172830148293\n",
      "Test-Epoch:90 | Loss:1.6294357473433387 | Fidelity:0.6176601241056028\n",
      "Epoch:95 | Loss:1.6134294364050266 | Fidelity:0.6235759783806586\n",
      "Test-Epoch:95 | Loss:1.6264868216311976 | Fidelity:0.6187988032240476\n",
      "Epoch:100 | Loss:1.6105754885876484 | Fidelity:0.6247086251557556\n",
      "Test-Epoch:100 | Loss:1.6235528362385303 | Fidelity:0.619929356207034\n",
      "Epoch:105 | Loss:1.607797138672407 | Fidelity:0.6258108235261783\n",
      "Test-Epoch:105 | Loss:1.6206540066017063 | Fidelity:0.6210455663806443\n",
      "Epoch:110 | Loss:1.6051030454079425 | Fidelity:0.6268796110816897\n",
      "Test-Epoch:110 | Loss:1.617805434473499 | Fidelity:0.6221425775145266\n",
      "Epoch:115 | Loss:1.6024974167089556 | Fidelity:0.6279134337909767\n",
      "Test-Epoch:115 | Loss:1.6150169388698492 | Fidelity:0.6232171224638118\n",
      "Epoch:120 | Loss:1.5999815687143353 | Fidelity:0.6289117132765394\n",
      "Test-Epoch:120 | Loss:1.6122942549349963 | Fidelity:0.6242672289820633\n",
      "Epoch:125 | Loss:1.5975558592018178 | Fidelity:0.6298742384906487\n",
      "Test-Epoch:125 | Loss:1.6096409786723234 | Fidelity:0.6252916179754819\n",
      "Epoch:130 | Loss:1.5952214543113201 | Fidelity:0.6308005759836531\n",
      "Test-Epoch:130 | Loss:1.6070606747017675 | Fidelity:0.6262890053022271\n",
      "Epoch:135 | Loss:1.5929815259178703 | Fidelity:0.6316896493376393\n",
      "Test-Epoch:135 | Loss:1.6045586024641025 | Fidelity:0.6272575088366552\n",
      "Epoch:140 | Loss:1.5908416473277405 | Fidelity:0.6325395800582051\n",
      "Test-Epoch:140 | Loss:1.6021426315522953 | Fidelity:0.6281943219913501\n",
      "Epoch:145 | Loss:1.5888093653721935 | Fidelity:0.633347807976213\n",
      "Test-Epoch:145 | Loss:1.599823157629308 | Fidelity:0.6290957287910438\n",
      "Epoch:150 | Loss:1.5868931398506734 | Fidelity:0.6341114334056666\n",
      "Test-Epoch:150 | Loss:1.5976121367306377 | Fidelity:0.6299574266097087\n",
      "Epoch:155 | Loss:1.585100989228551 | Fidelity:0.6348276695207556\n",
      "Test-Epoch:155 | Loss:1.5955216009221598 | Fidelity:0.6307750351653533\n",
      "Epoch:160 | Loss:1.5834392127227865 | Fidelity:0.6354942795904278\n",
      "Test-Epoch:160 | Loss:1.5935621047444304 | Fidelity:0.6315446379189685\n",
      "Epoch:165 | Loss:1.5819114727951764 | Fidelity:0.636109900630672\n",
      "Test-Epoch:165 | Loss:1.591741473051868 | Fidelity:0.6322632271545617\n",
      "Epoch:170 | Loss:1.5805183679014656 | Fidelity:0.6366742064139078\n",
      "Test-Epoch:170 | Loss:1.5900640480984507 | Fidelity:0.632928982524935\n",
      "Epoch:175 | Loss:1.5792574697089936 | Fidelity:0.6371879157953978\n",
      "Test-Epoch:175 | Loss:1.5885304566097034 | Fidelity:0.6335413737796087\n",
      "Epoch:180 | Loss:1.5781236919614268 | Fidelity:0.6376526892471578\n",
      "Test-Epoch:180 | Loss:1.5871377946643515 | Fidelity:0.6341011209640945\n",
      "Epoch:185 | Loss:1.577109819085151 | Fidelity:0.638070970081345\n",
      "Test-Epoch:185 | Loss:1.5858800757814118 | Fidelity:0.6346100633961764\n",
      "Epoch:190 | Loss:1.576207043481729 | Fidelity:0.6384458196664616\n",
      "Test-Epoch:190 | Loss:1.584748793523488 | Fidelity:0.6350709863164233\n",
      "Epoch:195 | Loss:1.575405416888307 | Fidelity:0.6387807766352198\n",
      "Test-Epoch:195 | Loss:1.5837334906041804 | Fidelity:0.6354874398707645\n",
      "Epoch:200 | Loss:1.574694185382632 | Fidelity:0.6390797485807258\n",
      "Test-Epoch:200 | Loss:1.5828222782847061 | Fidelity:0.6358635676734036\n",
      "Epoch:205 | Loss:1.5740620285651832 | Fidelity:0.6393469282970681\n",
      "Test-Epoch:205 | Loss:1.582002295010706 | Fidelity:0.636203947805025\n",
      "Epoch:210 | Loss:1.5734972511500147 | Fidelity:0.6395867183607586\n",
      "Test-Epoch:210 | Loss:1.5812601218267717 | Fidelity:0.6365134407151776\n",
      "Epoch:215 | Loss:1.5729879792253494 | Fidelity:0.6398036475014873\n",
      "Test-Epoch:215 | Loss:1.580582181499004 | Fidelity:0.6367970364336591\n",
      "Epoch:220 | Loss:1.5725223996576696 | Fidelity:0.6400022675308137\n",
      "Test-Epoch:220 | Loss:1.5799551418071178 | Fidelity:0.6370596962615611\n",
      "Epoch:225 | Loss:1.5720890581987046 | Fidelity:0.6401870276044623\n",
      "Test-Epoch:225 | Loss:1.579366328109567 | Fidelity:0.6373061893308405\n",
      "Epoch:230 | Loss:1.5716772080323584 | Fidelity:0.6403621306166578\n",
      "Test-Epoch:230 | Loss:1.5788041334228893 | Fidelity:0.6375409298166186\n",
      "Epoch:235 | Loss:1.5712771812276642 | Fidelity:0.6405313828765948\n",
      "Test-Epoch:235 | Loss:1.5782584010418672 | Fidelity:0.6377678246929736\n",
      "Epoch:240 | Loss:1.5708807433850567 | Fidelity:0.6406980521574177\n",
      "Test-Epoch:240 | Loss:1.5777207475469952 | Fidelity:0.6379901441101145\n",
      "Epoch:245 | Loss:1.5704813876389707 | Fidelity:0.6408647504281803\n",
      "Test-Epoch:245 | Loss:1.577184793609894 | Fidelity:0.6382104265072055\n",
      "Epoch:250 | Loss:1.5700745284631714 | Fidelity:0.641033355880636\n",
      "Test-Epoch:250 | Loss:1.5766462762866853 | Fidelity:0.6384304283464985\n",
      "Epoch:255 | Loss:1.5696575678375808 | Fidelity:0.6412049843471269\n",
      "Test-Epoch:255 | Loss:1.576103028601478 | Fidelity:0.6386511240120746\n",
      "Epoch:260 | Loss:1.5692298239842792 | Fidelity:0.6413800136338152\n",
      "Test-Epoch:260 | Loss:1.5755548279362461 | Fidelity:0.6388727556543241\n",
      "Epoch:265 | Loss:1.5687923321321122 | Fidelity:0.6415581570920504\n",
      "Test-Epoch:265 | Loss:1.5750031306692083 | Fidelity:0.6390949267927032\n",
      "Epoch:270 | Loss:1.5683475432752454 | Fidelity:0.6417385765501087\n",
      "Test-Epoch:270 | Loss:1.5744507231571816 | Fidelity:0.6393167286413088\n",
      "Epoch:275 | Loss:1.5678989574526896 | Fidelity:0.6419200207478438\n",
      "Test-Epoch:275 | Loss:1.5739013262014412 | Fidelity:0.6395368853305303\n",
      "Epoch:280 | Loss:1.567450731546513 | Fidelity:0.6421009741037901\n",
      "Test-Epoch:280 | Loss:1.5733591910040685 | Fidelity:0.6397539037320203\n",
      "Epoch:285 | Loss:1.567007298757404 | Fidelity:0.642279801706169\n",
      "Test-Epoch:285 | Loss:1.572828720290176 | Fidelity:0.6399662151158267\n",
      "Epoch:290 | Loss:1.566573029685798 | Fidelity:0.6424548791290796\n",
      "Test-Epoch:290 | Loss:1.5723141405433418 | Fidelity:0.6401722987042093\n",
      "Epoch:295 | Loss:1.566151955496208 | Fidelity:0.6426246992129114\n",
      "Test-Epoch:295 | Loss:1.5718192420689017 | Fidelity:0.6403707806164769\n",
      "Epoch:300 | Loss:1.565747563822774 | Fidelity:0.6427879516220123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-Epoch:300 | Loss:1.5713471944076665 | Fidelity:0.6405605051334174\n",
      "Epoch:305 | Loss:1.5653626692011722 | Fidelity:0.6429435743184715\n",
      "Test-Epoch:305 | Loss:1.570900436535207 | Fidelity:0.6407405782453175\n",
      "Epoch:310 | Loss:1.5649993526897674 | Fidelity:0.6430907787766686\n",
      "Test-Epoch:310 | Loss:1.5704806349314853 | Fidelity:0.6409103858470726\n",
      "Epoch:315 | Loss:1.5646589603546734 | Fidelity:0.6432290526694018\n",
      "Test-Epoch:315 | Loss:1.5700886982795697 | Fidelity:0.641069590600146\n",
      "Epoch:320 | Loss:1.564342147495783 | Fidelity:0.6433581448582806\n",
      "Test-Epoch:320 | Loss:1.5697248352669166 | Fidelity:0.6412181123851025\n",
      "Epoch:325 | Loss:1.5640489546803722 | Fidelity:0.6434780378858543\n",
      "Test-Epoch:325 | Loss:1.5693886415125329 | Fidelity:0.6413560974913003\n",
      "Epoch:330 | Loss:1.563778902438838 | Fidelity:0.6435889129293\n",
      "Test-Epoch:330 | Loss:1.5690792026591245 | Fidelity:0.6414838813628829\n",
      "Epoch:335 | Loss:1.5635310933700912 | Fidelity:0.6436911115114426\n",
      "Test-Epoch:335 | Loss:1.5687952026871919 | Fidelity:0.6416019490096981\n",
      "Epoch:340 | Loss:1.5633043128938475 | Fidelity:0.6437850973599972\n",
      "Test-Epoch:340 | Loss:1.5685350290500173 | Fidelity:0.6417108962731038\n",
      "Epoch:345 | Loss:1.5630971225233175 | Fidelity:0.6438714208283994\n",
      "Test-Epoch:345 | Loss:1.5682968688702323 | Fidelity:0.6418113941673659\n",
      "Epoch:350 | Loss:1.5629079419738394 | Fidelity:0.6439506873718375\n",
      "Test-Epoch:350 | Loss:1.5680787928579016 | Fidelity:0.6419041576192677\n",
      "Epoch:355 | Loss:1.5627351184547695 | Fidelity:0.6440235307954254\n",
      "Test-Epoch:355 | Loss:1.5678788256023564 | Fidelity:0.6419899191804754\n",
      "Epoch:360 | Loss:1.5625769830172893 | Fidelity:0.6440905913985222\n",
      "Test-Epoch:360 | Loss:1.5676950023657494 | Fidelity:0.6420694077257393\n",
      "Epoch:365 | Loss:1.5624318948455143 | Fidelity:0.6441524987356396\n",
      "Test-Epoch:365 | Loss:1.5675254134653247 | Fidelity:0.6421433317779637\n",
      "Epoch:370 | Loss:1.5622982749437506 | Fidelity:0.6442098584802374\n",
      "Test-Epoch:370 | Loss:1.567368237838501 | Fidelity:0.6422123668968662\n",
      "Epoch:375 | Loss:1.5621746308845712 | Fidelity:0.6442632427791182\n",
      "Test-Epoch:375 | Loss:1.5672217675398628 | Fidelity:0.6422771464958893\n",
      "Epoch:380 | Loss:1.5620595742460341 | Fidelity:0.6443131834836958\n",
      "Test-Epoch:380 | Loss:1.5670844248321059 | Fidelity:0.6423382554724756\n",
      "Epoch:385 | Loss:1.5619518321836972 | Fidelity:0.6443601677022113\n",
      "Test-Epoch:385 | Loss:1.566954773310037 | Fidelity:0.6423962261108046\n",
      "Epoch:390 | Loss:1.5618502543348276 | Fidelity:0.6444046352036148\n",
      "Test-Epoch:390 | Loss:1.5668315242218986 | Fidelity:0.6424515358125729\n",
      "Epoch:395 | Loss:1.561753816000033 | Fidelity:0.6444469772955153\n",
      "Test-Epoch:395 | Loss:1.5667135388883264 | Fidelity:0.6425046063067239\n",
      "Epoch:400 | Loss:1.5616616183271324 | Fidelity:0.6444875368812972\n",
      "Test-Epoch:400 | Loss:1.566599827900973 | Fidelity:0.6425558040698385\n",
      "Epoch:405 | Loss:1.5615728860513314 | Fidelity:0.6445266094675258\n",
      "Test-Epoch:405 | Loss:1.5664895476252396 | Fidelity:0.6426054417488739\n",
      "Epoch:410 | Loss:1.5614869632256427 | Fidelity:0.6445644449410609\n",
      "Test-Epoch:410 | Loss:1.5663819944316901 | Fidelity:0.642653780417621\n",
      "Epoch:415 | Loss:1.561403307299343 | Fidelity:0.64460124996762\n",
      "Test-Epoch:415 | Loss:1.566276597027513 | Fidelity:0.6427010325210692\n",
      "Epoch:420 | Loss:1.5613214818580068 | Fidelity:0.644637190883902\n",
      "Test-Epoch:420 | Loss:1.566172907236898 | Fidelity:0.6427473653733154\n",
      "Epoch:425 | Loss:1.561241148313979 | Fidelity:0.6446723969681076\n",
      "Test-Epoch:425 | Loss:1.5660705895720255 | Fidelity:0.6427929050800976\n",
      "Epoch:430 | Loss:1.5611620568219706 | Fidelity:0.6447069639819024\n",
      "Test-Epoch:430 | Loss:1.565969409933678 | Fidelity:0.6428377407602796\n",
      "Epoch:435 | Loss:1.5610840366820178 | Fidelity:0.6447409578838385\n",
      "Test-Epoch:435 | Loss:1.5658692237729053 | Fidelity:0.6428819289451205\n",
      "Epoch:440 | Loss:1.5610069864786633 | Fidelity:0.644774418620972\n",
      "Test-Epoch:440 | Loss:1.5657699640303222 | Fidelity:0.6429254980407494\n",
      "Epoch:445 | Loss:1.5609308641871555 | Fidelity:0.6448073639133\n",
      "Test-Epoch:445 | Loss:1.5656716291450774 | Fidelity:0.6429684527489836\n",
      "Epoch:450 | Loss:1.5608556774562197 | Fidelity:0.644839792954325\n",
      "Test-Epoch:450 | Loss:1.5655742713938718 | Fidelity:0.6430107783535286\n",
      "Epoch:455 | Loss:1.5607814742510229 | Fidelity:0.6448716899611042\n",
      "Test-Epoch:455 | Loss:1.5654779857821282 | Fidelity:0.643052444792687\n",
      "Epoch:460 | Loss:1.5607083340122698 | Fidelity:0.6449030275175908\n",
      "Test-Epoch:460 | Loss:1.5653828996685717 | Fidelity:0.6430934104545036\n",
      "Epoch:465 | Loss:1.5606363594574082 | Fidelity:0.6449337696662433\n",
      "Test-Epoch:465 | Loss:1.5652891632619075 | Fidelity:0.6431336256456199\n",
      "Epoch:470 | Loss:1.5605656691201033 | Fidelity:0.6449638747138436\n",
      "Test-Epoch:470 | Loss:1.565196941087193 | Fidelity:0.6431730356998281\n",
      "Epoch:475 | Loss:1.5604963906942555 | Fidelity:0.644993297728429\n",
      "Test-Epoch:475 | Loss:1.5651064044803575 | Fidelity:0.6432115837063418\n",
      "Epoch:480 | Loss:1.5604286552209465 | Fidelity:0.6450219927144367\n",
      "Test-Epoch:480 | Loss:1.5650177251342001 | Fidelity:0.6432492128503453\n",
      "Epoch:485 | Loss:1.5603625921304818 | Fidelity:0.645049914462756\n",
      "Test-Epoch:485 | Loss:1.564931069688004 | Fidelity:0.6432858683695399\n",
      "Epoch:490 | Loss:1.5602983251284577 | Fidelity:0.6450770200808542\n",
      "Test-Epoch:490 | Loss:1.5648465953267154 | Fidelity:0.64332149913979\n",
      "Epoch:495 | Loss:1.560235968894603 | Fidelity:0.6451032702155327\n",
      "Test-Epoch:495 | Loss:1.5647644463341093 | Fidelity:0.6433560589106816\n",
      "Epoch:500 | Loss:1.5601756265467837 | Fidelity:0.6451286299868559\n",
      "Test-Epoch:500 | Loss:1.5646847515281415 | Fidelity:0.6433895072176291\n",
      "Epoch:505 | Loss:1.560117387809281 | Fidelity:0.6451530696566387\n",
      "Test-Epoch:505 | Loss:1.5646076224945424 | Fidelity:0.6434218100015512\n",
      "Epoch:510 | Loss:1.560061327815504 | Fidelity:0.64517656505816\n",
      "Test-Epoch:510 | Loss:1.5645331525275852 | Fidelity:0.6434529399696821\n",
      "Epoch:515 | Loss:1.5600075064691785 | Fidelity:0.6451990978159833\n",
      "Test-Epoch:515 | Loss:1.5644614161831316 | Fidelity:0.6434828767324894\n",
      "Epoch:520 | Loss:1.5599559682856041 | Fidelity:0.6452206553856348\n",
      "Test-Epoch:520 | Loss:1.5643924693491058 | Fidelity:0.6435116067516367\n",
      "Epoch:525 | Loss:1.5599067426345326 | Fidelity:0.6452412309428591\n",
      "Test-Epoch:525 | Loss:1.5643263497409987 | Fidelity:0.6435391231330351\n",
      "Epoch:530 | Loss:1.5598598443088765 | Fidelity:0.6452608231511485\n",
      "Test-Epoch:530 | Loss:1.5642630777350897 | Fidelity:0.6435654252971701\n",
      "Epoch:535 | Loss:1.5598152743476614 | Fidelity:0.6452794358346173\n",
      "Test-Epoch:535 | Loss:1.5642026574585568 | Fidelity:0.6435905185565172\n",
      "Epoch:540 | Loss:1.5597730210475962 | Fidelity:0.645297077581046\n",
      "Test-Epoch:540 | Loss:1.5641450780635515 | Fidelity:0.643614413626948\n",
      "Epoch:545 | Loss:1.5597330611040494 | Fidelity:0.6453137612974646\n",
      "Test-Epoch:545 | Loss:1.5640903151206376 | Fidelity:0.6436371260969795\n",
      "Epoch:550 | Loss:1.559695360829886 | Fidelity:0.6453295037377487\n",
      "Test-Epoch:550 | Loss:1.5640383320760345 | Fidelity:0.6436586758753667\n",
      "Epoch:555 | Loss:1.5596598774078496 | Fidelity:0.6453443250189559\n",
      "Test-Epoch:555 | Loss:1.5639890817257962 | Fidelity:0.6436790866343416\n",
      "Epoch:560 | Loss:1.559626560139572 | Fidelity:0.645358248140321\n",
      "Test-Epoch:560 | Loss:1.5639425076684046 | Fidelity:0.6436983852627047\n",
      "Epoch:565 | Loss:1.559595351661575 | Fidelity:0.6453712985160708\n",
      "Test-Epoch:565 | Loss:1.563898545705485 | Fidelity:0.6437166013399213\n",
      "Epoch:570 | Loss:1.559566189105083 | Fidelity:0.645383503530772\n",
      "Test-Epoch:570 | Loss:1.5638571251674656 | Fidelity:0.6437337666397387\n",
      "Epoch:575 | Loss:1.5595390051824476 | Fidelity:0.6453948921236463\n",
      "Test-Epoch:575 | Loss:1.5638181701475415 | Fidelity:0.643749914669408\n",
      "Epoch:580 | Loss:1.5595137291885472 | Fidelity:0.6454054944061953\n",
      "Test-Epoch:580 | Loss:1.5637816006332423 | Fidelity:0.6437650802483803\n",
      "Epoch:585 | Loss:1.559490287909812 | Fidelity:0.6454153413158319\n",
      "Test-Epoch:585 | Loss:1.5637473335294754 | Fidelity:0.6437792991286561\n",
      "Epoch:590 | Loss:1.5594686064377152 | Fidelity:0.6454244643066472\n",
      "Test-Epoch:590 | Loss:1.563715283571298 | Fidelity:0.6437926076573265\n",
      "Epoch:595 | Loss:1.5594486088866268 | Fidelity:0.6454328950772645\n",
      "Test-Epoch:595 | Loss:1.5636853641277906 | Fidelity:0.6438050424806759\n",
      "Epoch:600 | Loss:1.559430219018519 | Fidelity:0.6454406653347682\n",
      "Test-Epoch:600 | Loss:1.563657487901057 | Fidelity:0.6438166402882635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:605 | Loss:1.5594133607789666 | Fidelity:0.6454478065929437\n",
      "Test-Epoch:605 | Loss:1.563631567526287 | Fidelity:0.6438274375946349\n",
      "Epoch:610 | Loss:1.5593979587502336 | Fidelity:0.64545435000256\n",
      "Test-Epoch:610 | Loss:1.5636075160800944 | Fidelity:0.6438374705558995\n",
      "Epoch:615 | Loss:1.5593839385283528 | Fidelity:0.6454603262110079\n",
      "Test-Epoch:615 | Loss:1.5635852475053813 | Fidelity:0.6438467748179763\n",
      "Epoch:620 | Loss:1.5593712270313136 | Fidelity:0.6454657652485267\n",
      "Test-Epoch:620 | Loss:1.5635646769610496 | Fidelity:0.6438553853933214\n",
      "Epoch:625 | Loss:1.559359752746073 | Fidelity:0.6454706964380249\n",
      "Test-Epoch:625 | Loss:1.5635457211053636 | Fidelity:0.6438633365627571\n",
      "Epoch:630 | Loss:1.559349445921722 | Fidelity:0.6454751483256538\n",
      "Test-Epoch:630 | Loss:1.5635282983212617 | Fidelity:0.6438706617992055\n",
      "Epoch:635 | Loss:1.5593402387160111 | Fidelity:0.6454791486293376\n",
      "Test-Epoch:635 | Loss:1.5635123288916126 | Fidelity:0.6438773937102765\n",
      "Epoch:640 | Loss:1.5593320653020348 | Fidelity:0.6454827242026205\n",
      "Test-Epoch:640 | Loss:1.563497735131939 | Fidelity:0.6438835639968008\n",
      "Epoch:645 | Loss:1.5593248619413176 | Fidelity:0.6454859010114032\n",
      "Test-Epoch:645 | Loss:1.5634844414873768 | Fidelity:0.6438892034247112\n",
      "Epoch:650 | Loss:1.559318567028947 | Fidelity:0.6454887041213755\n",
      "Test-Epoch:650 | Loss:1.5634723745999883 | Fidelity:0.6438943418079143\n",
      "Epoch:655 | Loss:1.5593131211159459 | Fidelity:0.645491157694114\n",
      "Test-Epoch:655 | Loss:1.563461463351962 | Fidelity:0.6438990080000121\n",
      "Epoch:660 | Loss:1.5593084669132298 | Fidelity:0.6454932849901536\n",
      "Test-Epoch:660 | Loss:1.5634516388892872 | Fidelity:0.6439032298930956\n",
      "Epoch:665 | Loss:1.5593045492811788 | Fidelity:0.6454951083774506\n",
      "Test-Epoch:665 | Loss:1.563442834630106 | Fidelity:0.6439070344219889\n",
      "Epoch:670 | Loss:1.5593013152082176 | Fidelity:0.6454966493439069\n",
      "Test-Epoch:670 | Loss:1.5634349862612518 | Fidelity:0.6439104475725709\n"
     ]
    }
   ],
   "source": [
    "loss_hist=[]\n",
    "fid_hist=[]\n",
    "\n",
    "loss_hist_test=[]\n",
    "fid_hist_test=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batches = iterate_batches(X=training_data, batch_size=batch_size)\n",
    "    for xbatch in batches:\n",
    "        encoder_params = opt.step(cost, encoder_params, X=xbatch)\n",
    "\n",
    "\n",
    "    \n",
    "    if epoch%5 == 0:\n",
    "        \n",
    "        loss_training = cost(encoder_params, X_training )\n",
    "        fidel = fidelity(encoder_params, X_training )\n",
    "        \n",
    "        loss_hist.append(loss_training)\n",
    "        fid_hist.append(fidel)\n",
    "        print(\"Epoch:{} | Loss:{} | Fidelity:{}\".format(epoch, loss_training, fidel))\n",
    "\n",
    "        loss_test = cost(encoder_params, X_tes )\n",
    "        fidel = fidelity(encoder_params, X_tes )\n",
    "        loss_hist_test.append(loss_test)\n",
    "        fid_hist_test.append(fidel)\n",
    "        print(\"Test-Epoch:{} | Loss:{} | Fidelity:{}\".format(epoch, loss_test, fidel))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rezults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x for x in range(0,len(loss_hist)*5,5)],np.array(fid_hist),label=\"train fid\")\n",
    "plt.plot([x for x in range(0,len(loss_hist)*5,5)],np.array(fid_hist_test),label=\"test fid\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"6-2-6:fidelity e4\",)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"fid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x for x in range(0,len(loss_hist)*5,5)],np.array(loss_hist),label=\"train loss\")\n",
    "plt.plot([x for x in range(0,len(loss_hist)*5,5)],np.array(loss_hist_test),label=\"test loss\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"6-2-6:loss e4\",)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"training_e4\"\n",
    "\n",
    "Circuit_prop={   \"shots\":shots, \"nr_trash\":nr_trash, \"nr_latent\":nr_latent ,\"nr_ent\":nr_ent ,\"ent_state\":ent_state }\n",
    "Training_param = { \"num_samples\" : num_samples,\n",
    "                    \"batch_size\" :batch_size,\n",
    "                    \"nr_layers\":nr_layers,\n",
    "                    \"epochs\" :epochs,\n",
    "                    \"learning_rate\" : learning_rate ,\n",
    "                    \"beta1\" : beta1,\n",
    "                    \"beta2 \":beta2,\n",
    "                     \"optimizer\":\"Adam\"}\n",
    "\n",
    "performance={\"loss_hist\":loss_hist, \"fid_hist\":fid_hist,\n",
    "             \"loss_hist_test\":loss_hist_test, \"fid_hist_test\":fid_hist_test,\n",
    "             \"encoder_params\":encoder_params}\n",
    "\n",
    "experiment_data={\"Circuit_prop\":Circuit_prop,\n",
    "                \"Training_param\":Training_param,\n",
    "                \"performance:\":performance,\n",
    "                \"Name\":name}\n",
    "\n",
    "# open file for writing\n",
    "f = open(name+\".txt\",\"w\")\n",
    "f.write( str(experiment_data) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
